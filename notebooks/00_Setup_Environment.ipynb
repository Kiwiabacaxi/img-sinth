{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-title"
   },
   "source": [
    "# üå± Setup Ambiente - Gerador de Pastagens Brasileiras\n",
    "\n",
    "Este notebook configura o ambiente completo para gera√ß√£o de imagens sint√©ticas de pastagens brasileiras usando Stable Diffusion.\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- ‚úÖ Otimizado para Google Colab (GPU T4/V100)\n",
    "- ‚úÖ Instala√ß√£o autom√°tica de depend√™ncias\n",
    "- ‚úÖ Verifica√ß√£o de GPU e recursos\n",
    "- ‚úÖ Download de modelos base\n",
    "- ‚úÖ Configura√ß√£o de cache otimizado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "requirements-section"
   },
   "source": [
    "## üîß 1. Verifica√ß√£o de Sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "system-check"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import platform\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üñ•Ô∏è  INFORMA√á√ïES DO SISTEMA\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"CPU cores: {psutil.cpu_count()}\")\n",
    "print(f\"RAM total: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "print(f\"RAM dispon√≠vel: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "\n",
    "print(\"\\nüöÄ INFORMA√á√ïES GPU\")\n",
    "print(\"=\" * 50)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA dispon√≠vel: {torch.version.cuda}\")\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ Mem√≥ria GPU: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "    print(f\"‚úÖ Compute Capability: {torch.cuda.get_device_capability(0)}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA n√£o dispon√≠vel - funcionar√° apenas em CPU\")\n",
    "    print(\"‚ö†Ô∏è  Para melhor performance, use runtime GPU no Colab\")\n",
    "\n",
    "print(\"\\nüìÅ DIRET√ìRIOS COLAB\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Working dir: {Path.cwd()}\")\n",
    "if Path('/content').exists():\n",
    "    print(\"‚úÖ Executando no Google Colab\")\n",
    "    print(f\"Espa√ßo dispon√≠vel: {psutil.disk_usage('/content').free / (1024**3):.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  N√£o detectado Google Colab - ajuste paths se necess√°rio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone-repo-section"
   },
   "source": [
    "## üì• 2. Clone do Reposit√≥rio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Verificar se j√° est√° clonado\n",
    "if Path('brazilian-pasture-synthesis').exists():\n",
    "    print(\"‚úÖ Reposit√≥rio j√° existe - atualizando...\")\n",
    "    %cd brazilian-pasture-synthesis\n",
    "    !git pull origin main\n",
    "else:\n",
    "    print(\"üì• Clonando reposit√≥rio...\")\n",
    "    # SUBSTITUA pela URL real do seu reposit√≥rio\n",
    "    !git clone https://github.com/seu-usuario/brazilian-pasture-synthesis.git\n",
    "    %cd brazilian-pasture-synthesis\n",
    "\n",
    "# Verificar estrutura\n",
    "print(\"\\nüìÇ ESTRUTURA DO PROJETO:\")\n",
    "!find . -maxdepth 2 -type d | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-deps-section"
   },
   "source": [
    "## üîß 3. Instala√ß√£o de Depend√™ncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Instalar depend√™ncias do requirements.txt\n",
    "print(\"üì¶ Instalando depend√™ncias principais...\")\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "print(\"\\nüîß Instalando depend√™ncias espec√≠ficas do Colab...\")\n",
    "# xFormers para otimiza√ß√£o de mem√≥ria\n",
    "!pip install xformers==0.0.22.post7 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Depend√™ncias de qualidade de imagem\n",
    "!pip install lpips pytorch-fid\n",
    "\n",
    "print(\"\\n‚úÖ Instala√ß√£o conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-dirs-section"
   },
   "source": [
    "## üìÅ 4. Configura√ß√£o de Diret√≥rios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-dirs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Diret√≥rios essenciais para Colab\n",
    "directories = [\n",
    "    \"/content/model_cache\",\n",
    "    \"/content/generated_cache\", \n",
    "    \"/content/outputs\",\n",
    "    \"/content/datasets\",\n",
    "    \"/content/temp\",\n",
    "    \"outputs/generated_images\",\n",
    "    \"outputs/datasets\", \n",
    "    \"outputs/models\",\n",
    "    \"outputs/evaluations\",\n",
    "    \"outputs/samples\"\n",
    "]\n",
    "\n",
    "print(\"üìÅ Criando estrutura de diret√≥rios...\")\n",
    "for directory in directories:\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"‚úÖ {directory}\")\n",
    "\n",
    "print(\"\\nüìä Espa√ßo em disco:\")\n",
    "if Path('/content').exists():\n",
    "    disk_usage = psutil.disk_usage('/content')\n",
    "    print(f\"Total: {disk_usage.total / (1024**3):.1f} GB\")\n",
    "    print(f\"Usado: {disk_usage.used / (1024**3):.1f} GB\")\n",
    "    print(f\"Livre: {disk_usage.free / (1024**3):.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "env-vars-section"
   },
   "source": [
    "## ‚öôÔ∏è 5. Configura√ß√£o de Vari√°veis de Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "env-vars"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configura√ß√µes de otimiza√ß√£o de mem√≥ria\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# Diret√≥rios de cache\n",
    "os.environ['HF_HOME'] = '/content/model_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/content/model_cache'\n",
    "os.environ['DIFFUSERS_CACHE'] = '/content/model_cache'\n",
    "\n",
    "# Configura√ß√µes espec√≠ficas\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "print(\"‚öôÔ∏è Vari√°veis de ambiente configuradas:\")\n",
    "important_vars = [\n",
    "    'PYTORCH_CUDA_ALLOC_CONF', 'HF_HOME', 'TRANSFORMERS_CACHE', \n",
    "    'DIFFUSERS_CACHE', 'CUDA_VISIBLE_DEVICES'\n",
    "]\n",
    "\n",
    "for var in important_vars:\n",
    "    print(f\"‚úÖ {var}: {os.environ.get(var, 'Not set')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-imports-section"
   },
   "source": [
    "## üß™ 6. Teste de Importa√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-imports"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"üß™ Testando importa√ß√µes principais...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Lista de importa√ß√µes cr√≠ticas\n",
    "imports_to_test = [\n",
    "    ('torch', 'PyTorch'),\n",
    "    ('torchvision', 'TorchVision'),\n",
    "    ('diffusers', 'Hugging Face Diffusers'),\n",
    "    ('transformers', 'Hugging Face Transformers'), \n",
    "    ('accelerate', 'Accelerate'),\n",
    "    ('controlnet_aux', 'ControlNet Auxiliary'),\n",
    "    ('ultralytics', 'YOLOv8/v9'),\n",
    "    ('cv2', 'OpenCV'),\n",
    "    ('PIL', 'Pillow'),\n",
    "    ('numpy', 'NumPy'),\n",
    "    ('matplotlib', 'Matplotlib'),\n",
    "    ('yaml', 'PyYAML'),\n",
    "    ('albumentations', 'Albumentations'),\n",
    "    ('tqdm', 'TQDM')\n",
    "]\n",
    "\n",
    "failed_imports = []\n",
    "\n",
    "for module, name in imports_to_test:\n",
    "    try:\n",
    "        __import__(module)\n",
    "        print(f\"‚úÖ {name:25} - OK\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå {name:25} - FAILED: {e}\")\n",
    "        failed_imports.append(module)\n",
    "\n",
    "# Testar importa√ß√µes espec√≠ficas do projeto\n",
    "print(\"\\nüîß Testando m√≥dulos do projeto...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "project_imports = [\n",
    "    ('src.diffusion.pipeline_manager', 'Pipeline Manager'),\n",
    "    ('src.diffusion.prompt_engine', 'Prompt Engine'), \n",
    "    ('src.dataset.generator', 'Dataset Generator'),\n",
    "    ('src.dataset.quality_metrics', 'Quality Metrics')\n",
    "]\n",
    "\n",
    "for module, name in project_imports:\n",
    "    try:\n",
    "        __import__(module)\n",
    "        print(f\"‚úÖ {name:25} - OK\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå {name:25} - FAILED: {e}\")\n",
    "        failed_imports.append(module)\n",
    "\n",
    "# Resumo\n",
    "if failed_imports:\n",
    "    print(f\"\\n‚ö†Ô∏è {len(failed_imports)} importa√ß√µes falharam:\")\n",
    "    for module in failed_imports:\n",
    "        print(f\"   - {module}\")\n",
    "    print(\"\\nüîß Execute a c√©lula de instala√ß√£o novamente se necess√°rio\")\n",
    "else:\n",
    "    print(\"\\nüéâ Todas as importa√ß√µes foram bem-sucedidas!\")\n",
    "    print(\"‚úÖ Sistema pronto para uso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu-test-section"
   },
   "source": [
    "## üöÄ 7. Teste de GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu-test"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"üß™ Testando performance da GPU...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    device = torch.cuda.current_device()\n",
    "    \n",
    "    # Teste b√°sico de aloca√ß√£o\n",
    "    print(\"üî∏ Teste 1: Aloca√ß√£o de mem√≥ria\")\n",
    "    try:\n",
    "        test_tensor = torch.randn(1000, 1000).cuda()\n",
    "        print(\"‚úÖ Aloca√ß√£o b√°sica: OK\")\n",
    "        del test_tensor\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro na aloca√ß√£o: {e}\")\n",
    "    \n",
    "    # Teste de opera√ß√µes\n",
    "    print(\"\\nüî∏ Teste 2: Opera√ß√µes matem√°ticas\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        a = torch.randn(2000, 2000).cuda()\n",
    "        b = torch.randn(2000, 2000).cuda() \n",
    "        c = torch.mm(a, b)\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"‚úÖ Multiplica√ß√£o de matrizes: {end_time - start_time:.3f}s\")\n",
    "        \n",
    "        del a, b, c\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro nas opera√ß√µes: {e}\")\n",
    "    \n",
    "    # Informa√ß√µes de mem√≥ria\n",
    "    print(\"\\nüî∏ Status da mem√≥ria GPU:\")\n",
    "    print(f\"   Alocada: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"   Reservada: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    print(f\"   M√°ximo alocado: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Reset stats\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    print(\"\\n‚úÖ GPU funcionando corretamente!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU n√£o dispon√≠vel - funcionar√° em CPU\")\n",
    "    print(\"Para melhor performance, ative GPU no Colab:\")\n",
    "    print(\"Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quick-download-section"
   },
   "source": [
    "## üì• 8. Download R√°pido de Modelos (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quick-download"
   },
   "outputs": [],
   "source": [
    "# Esta c√©lula faz download dos modelos principais - pode ser demorada\n",
    "# Execute apenas se quiser fazer cache dos modelos\n",
    "\n",
    "from diffusers import StableDiffusionXLPipeline, ControlNetModel\n",
    "import torch\n",
    "\n",
    "download_models = input(\"Fazer download dos modelos agora? (s/N): \")\n",
    "\n",
    "if download_models.lower() in ['s', 'sim', 'y', 'yes']:\n",
    "    print(\"üì• Fazendo download dos modelos base...\")\n",
    "    print(\"‚è≥ Isso pode demorar alguns minutos...\")\n",
    "    \n",
    "    try:\n",
    "        # Modelo principal Stable Diffusion XL\n",
    "        print(\"\\nüî∏ Baixando Stable Diffusion XL...\")\n",
    "        model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "        pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            cache_dir=\"/content/model_cache\",\n",
    "            torch_dtype=torch.float16,\n",
    "            variant=\"fp16\",\n",
    "            use_safetensors=True\n",
    "        )\n",
    "        print(\"‚úÖ Stable Diffusion XL baixado com sucesso!\")\n",
    "        del pipe  # Liberar mem√≥ria\n",
    "        \n",
    "        # Modelo ControlNet\n",
    "        print(\"\\nüî∏ Baixando ControlNet Canny...\")\n",
    "        controlnet = ControlNetModel.from_pretrained(\n",
    "            \"lllyasviel/sd-controlnet-canny\",\n",
    "            cache_dir=\"/content/model_cache\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        print(\"‚úÖ ControlNet Canny baixado com sucesso!\")\n",
    "        del controlnet\n",
    "        \n",
    "        # Limpar cache GPU\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        print(\"\\nüéâ Todos os modelos baixados com sucesso!\")\n",
    "        print(\"üíæ Modelos salvos em: /content/model_cache\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro no download: {e}\")\n",
    "        print(\"‚ö†Ô∏è Modelos ser√£o baixados conforme necess√°rio\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚è© Pulando download - modelos ser√£o baixados conforme necess√°rio\")\n",
    "    print(\"üí° Isso pode causar delay na primeira execu√ß√£o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-complete-section"
   },
   "source": [
    "## ‚úÖ 9. Verifica√ß√£o Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-complete"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "print(\"üîç VERIFICA√á√ÉO FINAL DO SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Verificar instala√ß√£o\n",
    "checks = [\n",
    "    (\"Python >= 3.8\", sys.version_info >= (3, 8)),\n",
    "    (\"PyTorch instalado\", 'torch' in sys.modules),\n",
    "    (\"Diffusers instalado\", 'diffusers' in sys.modules),\n",
    "    (\"CUDA dispon√≠vel\", torch.cuda.is_available()),\n",
    "    (\"Diret√≥rios criados\", Path('/content/model_cache').exists()),\n",
    "    (\"Projeto encontrado\", Path('src').exists()),\n",
    "    (\"Configs encontrados\", Path('configs').exists())\n",
    "]\n",
    "\n",
    "all_ok = True\n",
    "for check_name, result in checks:\n",
    "    status = \"‚úÖ\" if result else \"‚ùå\"\n",
    "    print(f\"{status} {check_name}\")\n",
    "    if not result:\n",
    "        all_ok = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if all_ok:\n",
    "    print(\"üéâ SETUP COMPLETO COM SUCESSO!\")\n",
    "    print(\"\")\n",
    "    print(\"üìö PR√ìXIMOS PASSOS:\")\n",
    "    print(\"1. Execute '01_Explore_Prompts.ipynb' para testar prompts\")\n",
    "    print(\"2. Use '02_Generate_Dataset.ipynb' para gerar imagens\")\n",
    "    print(\"3. Execute '03_Quality_Control.ipynb' para an√°lise de qualidade\")\n",
    "    print(\"\")\n",
    "    print(\"üí° DICA: Salve uma c√≥pia deste notebook no seu Drive!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è SETUP INCOMPLETO\")\n",
    "    print(\"Revise os passos anteriores e corrija os problemas\")\n",
    "    print(\"Em caso de d√∫vidas, consulte a documenta√ß√£o\")\n",
    "\n",
    "print(f\"\\nüïê Setup conclu√≠do em: {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting-section"
   },
   "source": [
    "## üõ†Ô∏è Troubleshooting\n",
    "\n",
    "### Problemas Comuns:\n",
    "\n",
    "**1. GPU n√£o detectada:**\n",
    "- V√° em `Runtime > Change runtime type > GPU`\n",
    "- Reinicie o runtime\n",
    "\n",
    "**2. Erro de mem√≥ria:**\n",
    "- Reinicie o runtime: `Runtime > Restart runtime`\n",
    "- Reduza batch_size nos experimentos\n",
    "\n",
    "**3. Erro de importa√ß√£o:**\n",
    "- Execute novamente a c√©lula de instala√ß√£o\n",
    "- Verifique se h√° conflitos de vers√µes\n",
    "\n",
    "**4. Download lento:**\n",
    "- Use servidor do Colab mais pr√≥ximo\n",
    "- Execute durante hor√°rios de menor uso\n",
    "\n",
    "### Comandos √öteis:\n",
    "```python\n",
    "# Verificar uso de GPU\n",
    "!nvidia-smi\n",
    "\n",
    "# Limpar cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Verificar espa√ßo em disco\n",
    "!df -h\n",
    "\n",
    "# Listar processos GPU\n",
    "!fuser -v /dev/nvidia*\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}