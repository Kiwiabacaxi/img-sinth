{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# üß™ Teste de Gera√ß√£o de Imagens - SYNTH_IMAGE\n",
    "\n",
    "Este notebook testa a gera√ß√£o de imagens sint√©ticas de pastagens brasileiras no Google Colab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üì¶ Instala√ß√£o e Configura√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# Instalar depend√™ncias faltantes\n",
    "!pip install controlnet-aux ultralytics xformers --upgrade --quiet\n",
    "\n",
    "# Verificar instala√ß√£o GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA dispon√≠vel: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM dispon√≠vel: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## üîß Importa√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libraries"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configurar device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Usando device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_setup"
   },
   "source": [
    "## ü§ñ Carregamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "# Carregar modelo Stable Diffusion\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "print(\"Carregando modelo Stable Diffusion...\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    safety_checker=None,\n",
    "    requires_safety_checker=False\n",
    ").to(device)\n",
    "\n",
    "# Usar scheduler mais r√°pido\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# Otimiza√ß√£o para Colab (vers√£o compat√≠vel)\n",
    "if device == \"cuda\":\n",
    "    try:\n",
    "        pipe.enable_model_cpu_offload()\n",
    "        print(\"CPU offload habilitado\")\n",
    "    except:\n",
    "        print(\"CPU offload n√£o dispon√≠vel\")\n",
    "\n",
    "print(\"Modelo carregado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generation"
   },
   "source": [
    "## üé® Gera√ß√£o de Imagens de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_images"
   },
   "outputs": [],
   "source": "# Prompts melhorados para pastagens brasileiras sem animais\nprompts = [\n    \"aerial view of empty brazilian pasture, green grass field, no animals, realistic, high quality, natural lighting\",\n    \"tropical grassland in brazil, savanna landscape, empty field, blue sky, photorealistic, no cattle, no animals\",\n    \"brazilian ranch pasture, green grass field, countryside, detailed, empty land, no animals, natural scenery\",\n    \"cerrado landscape brazil, grass field, scattered trees, natural lighting, empty pasture, no animals\",\n    \"green pasture field in brazil, tropical grass, open field, sunny day, no animals, rural landscape\",\n    \"brazilian grassland, natural grass texture, field view, clear sky, empty pasture, high resolution\",\n    \"tropical grass field brazil, savanna vegetation, natural landscape, no animals, realistic grass texture\",\n    \"empty brazilian pasture, grass coverage, rural scenery, natural lighting, field photography style\"\n]\n\n# Par√¢metros de gera√ß√£o\ngenerator = torch.Generator(device=device).manual_seed(42)\n\n# Gerar mais imagens para melhor an√°lise\nprint(f\"Gerando {len(prompts)} imagens de pastagens brasileiras...\")\nimages = []\n\nfor i, prompt in enumerate(prompts):\n    print(f\"Gerando imagem {i+1}/{len(prompts)}: {prompt[:60]}...\")\n    \n    image = pipe(\n        prompt=prompt,\n        negative_prompt=\"animals, cattle, cows, sheep, horses, people, humans, buildings, fences, low quality, blurry, distorted, ugly\",\n        num_inference_steps=25,  # Aumentado para melhor qualidade\n        guidance_scale=7.5,\n        width=512,\n        height=512,\n        generator=torch.Generator(device=device).manual_seed(42 + i)  # Seed diferente para cada imagem\n    ).images[0]\n    \n    images.append(image)\n    \n    # Mostrar progresso\n    if (i + 1) % 2 == 0:\n        print(f\"‚úÖ {i+1} imagens conclu√≠das\")\n\nprint(\"üéâ Todas as imagens geradas com sucesso!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "display"
   },
   "source": [
    "## üì∏ Visualiza√ß√£o dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display_images"
   },
   "outputs": [],
   "source": "# Exibir todas as imagens geradas em grade\nnum_images = len(images)\ncols = 4\nrows = (num_images + cols - 1) // cols  # Calcular linhas necess√°rias\n\nfig, axes = plt.subplots(rows, cols, figsize=(20, 5 * rows))\n\n# Se s√≥ tiver uma linha, axes pode n√£o ser 2D\nif rows == 1:\n    axes = axes.reshape(1, -1)\nif cols == 1:\n    axes = axes.reshape(-1, 1)\n\nfor i in range(num_images):\n    row = i // cols\n    col = i % cols\n    \n    axes[row, col].imshow(images[i])\n    axes[row, col].set_title(f\"Imagem {i+1}\\n{prompts[i][:40]}...\", fontsize=8)\n    axes[row, col].axis('off')\n\n# Esconder eixos vazios\nfor i in range(num_images, rows * cols):\n    row = i // cols\n    col = i % cols\n    axes[row, col].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Salvar todas as imagens\nprint(\"\\nüíæ Salvando imagens...\")\nfor i, image in enumerate(images):\n    filename = f\"pastagem_brasileira_{i+1:02d}.png\"\n    image.save(filename)\n    print(f\"‚úÖ {filename}\")\n\nprint(f\"\\nüéØ {len(images)} imagens de pastagens brasileiras salvas com sucesso!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_controlnet"
   },
   "source": [
    "## üéØ Teste ControlNet (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "controlnet_test"
   },
   "outputs": [],
   "source": [
    "# Testar ControlNet se dispon√≠vel\n",
    "try:\n",
    "    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "    from controlnet_aux import CannyDetector\n",
    "    import cv2\n",
    "    \n",
    "    print(\"‚úÖ ControlNet dispon√≠vel!\")\n",
    "    \n",
    "    # Criar uma imagem de borda simples para teste\n",
    "    canny = CannyDetector()\n",
    "    \n",
    "    # Usar primeira imagem gerada como base\n",
    "    if images:\n",
    "        canny_image = canny(images[0])\n",
    "        \n",
    "        # Exibir resultado\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(images[0])\n",
    "        plt.title(\"Imagem Original\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(canny_image, cmap='gray')\n",
    "        plt.title(\"Bordas Canny\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"ControlNet testado com sucesso!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ControlNet n√£o dispon√≠vel: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro no teste ControlNet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_yolo"
   },
   "source": [
    "## üéØ Teste YOLO (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yolo_test"
   },
   "outputs": [],
   "source": "# Testar YOLO com Segmenta√ß√£o de Gram√≠neas\ntry:\n    from ultralytics import YOLO\n    import cv2\n    import numpy as np\n    \n    print(\"‚úÖ YOLO dispon√≠vel!\")\n    \n    # Carregar modelo de segmenta√ß√£o YOLO (YOLOv8 segmentation)\n    print(\"Carregando modelo YOLOv8 para segmenta√ß√£o...\")\n    model = YOLO('yolov8n-seg.pt')  # Modelo de segmenta√ß√£o\n    \n    # Testar segmenta√ß√£o em m√∫ltiplas imagens\n    if images:\n        print(f\"Realizando segmenta√ß√£o em {min(4, len(images))} imagens...\")\n        \n        # Processar primeiras 4 imagens para an√°lise\n        for img_idx in range(min(4, len(images))):\n            print(f\"\\nüîç Processando imagem {img_idx+1}...\")\n            results = model(images[img_idx])\n            result = results[0]\n            \n            # Verificar se h√° m√°scaras de segmenta√ß√£o\n            if result.masks is not None:\n                # Converter imagem para numpy\n                img_array = np.array(images[img_idx])\n                \n                # Criar visualiza√ß√£o\n                fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n                fig.suptitle(f\"An√°lise YOLO - Imagem {img_idx+1}\", fontsize=14)\n                \n                # 1. Imagem original\n                axes[0, 0].imshow(images[img_idx])\n                axes[0, 0].set_title(\"Imagem Original\")\n                axes[0, 0].axis('off')\n                \n                # 2. Detec√ß√µes com caixas\n                annotated_img = result.plot()\n                axes[0, 1].imshow(cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB))\n                axes[0, 1].set_title(\"Detec√ß√µes YOLO\")\n                axes[0, 1].axis('off')\n                \n                # 3. M√°scaras de segmenta√ß√£o combinadas\n                masks = result.masks.data.cpu().numpy()\n                combined_mask = np.zeros(masks[0].shape)\n                \n                # Classes relacionadas a gram√≠neas/vegeta√ß√£o\n                grass_classes = []\n                for i, (mask, cls, conf) in enumerate(zip(masks, result.boxes.cls, result.boxes.conf)):\n                    class_name = model.names[int(cls)]\n                    confidence = float(conf)\n                    \n                    # Filtrar classes relacionadas √† vegeta√ß√£o/gram√≠neas\n                    vegetation_keywords = ['grass', 'plant', 'vegetation', 'field', 'lawn']\n                    if any(keyword in class_name.lower() for keyword in vegetation_keywords) or confidence > 0.3:\n                        combined_mask += mask\n                        grass_classes.append((class_name, confidence))\n                        print(f\"  Detectado: {class_name} (confian√ßa: {confidence:.2f})\")\n                \n                # Normalizar m√°scara\n                combined_mask = np.clip(combined_mask, 0, 1)\n                \n                axes[1, 0].imshow(combined_mask, cmap='Greens', alpha=0.8)  # Corrigido: 'green' -> 'Greens'\n                axes[1, 0].set_title(\"M√°scara de Gram√≠neas/Vegeta√ß√£o\")\n                axes[1, 0].axis('off')\n                \n                # 4. Overlay da m√°scara na imagem original\n                overlay = img_array.copy()\n                green_mask = np.zeros_like(img_array)\n                green_mask[:, :, 1] = combined_mask * 255  # Canal verde\n                \n                # Aplicar overlay\n                alpha = 0.4\n                overlay_result = cv2.addWeighted(img_array, 1-alpha, green_mask, alpha, 0)\n                \n                axes[1, 1].imshow(overlay_result)\n                axes[1, 1].set_title(\"Overlay: Gram√≠neas Detectadas\")\n                axes[1, 1].axis('off')\n                \n                plt.tight_layout()\n                plt.show()\n                \n                # Estat√≠sticas da segmenta√ß√£o\n                total_pixels = combined_mask.shape[0] * combined_mask.shape[1]\n                grass_pixels = np.sum(combined_mask > 0)\n                grass_percentage = (grass_pixels / total_pixels) * 100\n                \n                print(f\"  üìä An√°lise de Gram√≠neas - Imagem {img_idx+1}:\")\n                print(f\"    √Årea total: {total_pixels} pixels\")\n                print(f\"    √Årea de gram√≠neas: {grass_pixels} pixels\")\n                print(f\"    Cobertura: {grass_percentage:.1f}%\")\n                \n                if grass_classes:\n                    print(f\"    Classes: {grass_classes}\")\n                \n            else:\n                # Fallback: detec√ß√£o simples se n√£o houver m√°scaras\n                annotated_frame = result.plot()\n                plt.figure(figsize=(10, 6))\n                plt.imshow(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))\n                plt.title(f\"Detec√ß√µes YOLO - Imagem {img_idx+1} (sem segmenta√ß√£o)\")\n                plt.axis('off')\n                plt.show()\n                \n                print(f\"  ‚ö†Ô∏è Imagem {img_idx+1}: Modelo n√£o retornou m√°scaras de segmenta√ß√£o\")\n        \n        print(\"\\n‚úÖ An√°lise YOLO conclu√≠da para todas as imagens!\")\n    \nexcept ImportError as e:\n    print(f\"‚ùå YOLO n√£o dispon√≠vel: {e}\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Erro na segmenta√ß√£o YOLO: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "metrics"
   },
   "source": [
    "## üìä M√©tricas de Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## üå± Segmenta√ß√£o Avan√ßada de Gram√≠neas",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Segmenta√ß√£o espec√≠fica de gram√≠neas usando t√©cnicas de cor e textura\ndef segment_grass_advanced(image):\n    \"\"\"\n    Segmenta√ß√£o avan√ßada de gram√≠neas baseada em cor, textura e caracter√≠sticas visuais\n    \"\"\"\n    # Converter para numpy array\n    img_array = np.array(image)\n    \n    # Converter para HSV para melhor segmenta√ß√£o de cores\n    hsv = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)\n    \n    # Definir ranges de cor para gram√≠neas (tons de verde)\n    # Verde claro a escuro, considerando diferentes condi√ß√µes de ilumina√ß√£o\n    lower_green1 = np.array([35, 40, 40])   # Verde claro\n    upper_green1 = np.array([85, 255, 255])\n    \n    lower_green2 = np.array([25, 30, 30])   # Verde mais amarelado (grama seca)\n    upper_green2 = np.array([95, 255, 180])\n    \n    # Criar m√°scaras de cor\n    mask_green1 = cv2.inRange(hsv, lower_green1, upper_green1)\n    mask_green2 = cv2.inRange(hsv, lower_green2, upper_green2)\n    grass_mask = cv2.bitwise_or(mask_green1, mask_green2)\n    \n    # Opera√ß√µes morfol√≥gicas para refinar a m√°scara\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n    grass_mask = cv2.morphologyEx(grass_mask, cv2.MORPH_CLOSE, kernel)\n    grass_mask = cv2.morphologyEx(grass_mask, cv2.MORPH_OPEN, kernel)\n    \n    # Filtrar por √°rea (remover pequenos ru√≠dos)\n    contours, _ = cv2.findContours(grass_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    filtered_mask = np.zeros_like(grass_mask)\n    \n    min_area = 100  # √Årea m√≠nima para considerar como gram√≠nea\n    for contour in contours:\n        area = cv2.contourArea(contour)\n        if area > min_area:\n            cv2.fillPoly(filtered_mask, [contour], 255)\n    \n    return filtered_mask\n\n# Aplicar segmenta√ß√£o avan√ßada\nif images:\n    print(\"üå± Aplicando segmenta√ß√£o avan√ßada de gram√≠neas...\")\n    \n    # Segmentar primeira imagem\n    grass_mask = segment_grass_advanced(images[0])\n    \n    # Criar visualiza√ß√£o comparativa\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    \n    # Imagem original\n    axes[0, 0].imshow(images[0])\n    axes[0, 0].set_title(\"Imagem Original\")\n    axes[0, 0].axis('off')\n    \n    # M√°scara de gram√≠neas\n    axes[0, 1].imshow(grass_mask, cmap='Greens')\n    axes[0, 1].set_title(\"M√°scara de Gram√≠neas\")\n    axes[0, 1].axis('off')\n    \n    # Overlay colorido\n    img_array = np.array(images[0])\n    overlay = img_array.copy()\n    \n    # Aplicar cor verde nas √°reas de gram√≠nea\n    overlay[grass_mask > 0] = [0, 255, 0]  # Verde puro\n    blended = cv2.addWeighted(img_array, 0.7, overlay, 0.3, 0)\n    \n    axes[0, 2].imshow(blended)\n    axes[0, 2].set_title(\"Overlay: Gram√≠neas Destacadas\")\n    axes[0, 2].axis('off')\n    \n    # An√°lise por componentes de cor HSV\n    hsv = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)\n    \n    axes[1, 0].imshow(hsv[:,:,0], cmap='hsv')\n    axes[1, 0].set_title(\"Canal H (Matiz)\")\n    axes[1, 0].axis('off')\n    \n    axes[1, 1].imshow(hsv[:,:,1], cmap='gray')\n    axes[1, 1].set_title(\"Canal S (Satura√ß√£o)\")\n    axes[1, 1].axis('off')\n    \n    axes[1, 2].imshow(hsv[:,:,2], cmap='gray')\n    axes[1, 2].set_title(\"Canal V (Valor)\")\n    axes[1, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Estat√≠sticas detalhadas\n    total_pixels = grass_mask.shape[0] * grass_mask.shape[1]\n    grass_pixels = np.sum(grass_mask > 0)\n    grass_percentage = (grass_pixels / total_pixels) * 100\n    \n    print(f\"\\nüìä An√°lise Detalhada de Gram√≠neas:\")\n    print(f\"Resolu√ß√£o da imagem: {grass_mask.shape[1]}x{grass_mask.shape[0]}\")\n    print(f\"Total de pixels: {total_pixels:,}\")\n    print(f\"Pixels de gram√≠nea detectados: {grass_pixels:,}\")\n    print(f\"Cobertura de gram√≠neas: {grass_percentage:.2f}%\")\n    print(f\"√Årea de gram√≠neas: ~{(grass_pixels * 0.01):.0f} m¬≤ (estimativa)\")\n    \n    # An√°lise de qualidade da pastagem\n    if grass_percentage > 70:\n        quality = \"Excelente üü¢\"\n    elif grass_percentage > 50:\n        quality = \"Boa üü°\" \n    elif grass_percentage > 30:\n        quality = \"Regular üü†\"\n    else:\n        quality = \"Baixa üî¥\"\n    \n    print(f\"Qualidade da pastagem: {quality}\")\nelse:\n    print(\"‚ùå Nenhuma imagem dispon√≠vel para segmenta√ß√£o\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "performance_metrics"
   },
   "outputs": [],
   "source": [
    "# Informa√ß√µes de mem√≥ria\n",
    "if torch.cuda.is_available():\n",
    "    print(\"üìä Uso de Mem√≥ria GPU:\")\n",
    "    print(f\"Alocada: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"Reservada: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    print(f\"M√°xima alocada: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# Verificar tamanhos das imagens\n",
    "if images:\n",
    "    print(\"\\nüì∏ Informa√ß√µes das Imagens:\")\n",
    "    for i, image in enumerate(images):\n",
    "        print(f\"Imagem {i+1}: {image.size} - Modo: {image.mode}\")\n",
    "\n",
    "print(\"\\n‚úÖ Teste conclu√≠do com sucesso!\")\n",
    "print(\"\\nüéØ Pr√≥ximos passos:\")\n",
    "print(\"- Ajustar prompts para pastagens brasileiras espec√≠ficas\")\n",
    "print(\"- Implementar pipeline completo com ControlNet\")\n",
    "print(\"- Integrar detec√ß√£o YOLO para valida√ß√£o\")\n",
    "print(\"- Configurar dataset personalizado\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}