{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Controle de Qualidade - Pastagens Brasileiras\n",
    "\n",
    "Sistema avançado de análise de qualidade para imagens sintéticas de pastagens brasileiras.\n",
    "\n",
    "**Funcionalidades:**\n",
    "- ✅ Análise automática de qualidade técnica\n",
    "- ✅ Avaliação de realismo agronômico\n",
    "- ✅ Consistência sazonal e bioma\n",
    "- ✅ Comparação com datasets reais\n",
    "- ✅ Filtragem automática de baixa qualidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "if '/content/img-sinth' not in sys.path:\n",
    "    sys.path.append('/content/img-sinth')\n",
    "if not Path('src').exists():\n",
    "    os.chdir('/content/img-sinth')\n",
    "\n",
    "import torch, numpy as np, matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json, time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.dataset.quality_metrics import QualityMetrics\n",
    "from src.diffusion.image_postprocess import ImagePostProcessor\n",
    "\n",
    "print(\"✅ Setup concluído!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Adicionar projeto ao path\nif '/content/img-sinth' not in sys.path:\n    sys.path.append('/content/img-sinth')\n    \nif not Path('src').exists():\n    os.chdir('/content/img-sinth')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_quality(dataset_path, sample_size=50):\n",
    "    \"\"\"Analisa qualidade de um dataset completo\"\"\"\n",
    "    \n",
    "    dataset_dir = Path(dataset_path)\n",
    "    images_dir = dataset_dir / \"images\"\n",
    "    \n",
    "    if not images_dir.exists():\n",
    "        print(f\"❌ Diretório de imagens não encontrado: {images_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Inicializar avaliador\n",
    "    quality_evaluator = QualityMetrics()\n",
    "    \n",
    "    # Listar imagens\n",
    "    image_files = list(images_dir.glob(\"*.jpg\")) + list(images_dir.glob(\"*.png\"))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"❌ Nenhuma imagem encontrada\")\n",
    "        return\n",
    "    \n",
    "    # Amostra aleatória\n",
    "    if len(image_files) > sample_size:\n",
    "        import random\n",
    "        image_files = random.sample(image_files, sample_size)\n",
    "    \n",
    "    print(f\"🔍 Analisando {len(image_files)} imagens...\")\n",
    "    \n",
    "    # Análise das imagens\n",
    "    results = []\n",
    "    \n",
    "    for img_path in tqdm(image_files, desc=\"Analisando qualidade\"):\n",
    "        try:\n",
    "            # Carregar imagem\n",
    "            image = Image.open(img_path)\n",
    "            \n",
    "            # Tentar carregar metadados\n",
    "            metadata_path = dataset_dir / \"metadata\" / (img_path.stem + \".json\")\n",
    "            metadata = {}\n",
    "            if metadata_path.exists():\n",
    "                with open(metadata_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    pasture_config = data.get('pasture_config', {})\n",
    "                    metadata = {\n",
    "                        'biome': pasture_config.get('biome'),\n",
    "                        'season': pasture_config.get('season'),\n",
    "                        'quality': pasture_config.get('quality')\n",
    "                    }\n",
    "            \n",
    "            # Avaliar qualidade\n",
    "            report = quality_evaluator.evaluate_image_quality(image, metadata)\n",
    "            \n",
    "            results.append({\n",
    "                'filename': img_path.name,\n",
    "                'overall_score': report.overall_score,\n",
    "                'technical_quality': report.technical_quality,\n",
    "                'agricultural_realism': report.agricultural_realism,\n",
    "                'seasonal_consistency': report.seasonal_consistency,\n",
    "                'metadata': metadata\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar {img_path.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Análise dos resultados\n",
    "    if not results:\n",
    "        print(\"❌ Nenhum resultado obtido\")\n",
    "        return\n",
    "    \n",
    "    scores = {\n",
    "        'overall': [r['overall_score'] for r in results],\n",
    "        'technical': [r['technical_quality'] for r in results],\n",
    "        'agricultural': [r['agricultural_realism'] for r in results],\n",
    "        'seasonal': [r['seasonal_consistency'] for r in results]\n",
    "    }\n",
    "    \n",
    "    print(\"\\n📊 RESULTADOS DA ANÁLISE DE QUALIDADE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for metric_name, values in scores.items():\n",
    "        print(f\"\\n{metric_name.title()}:\")\n",
    "        print(f\"   Média: {np.mean(values):.3f} ± {np.std(values):.3f}\")\n",
    "        print(f\"   Min/Max: {np.min(values):.3f} / {np.max(values):.3f}\")\n",
    "        print(f\"   Mediana: {np.median(values):.3f}\")\n",
    "    \n",
    "    # Distribuição de qualidade\n",
    "    overall_scores = scores['overall']\n",
    "    excellent = sum(1 for s in overall_scores if s > 0.8)\n",
    "    good = sum(1 for s in overall_scores if 0.6 <= s <= 0.8)\n",
    "    moderate = sum(1 for s in overall_scores if 0.4 <= s < 0.6)\n",
    "    poor = sum(1 for s in overall_scores if s < 0.4)\n",
    "    \n",
    "    print(f\"\\n🎯 Distribuição Geral:\")\n",
    "    print(f\"   Excelente (>0.8): {excellent} ({excellent/len(results)*100:.1f}%)\")\n",
    "    print(f\"   Bom (0.6-0.8): {good} ({good/len(results)*100:.1f}%)\")\n",
    "    print(f\"   Moderado (0.4-0.6): {moderate} ({moderate/len(results)*100:.1f}%)\")\n",
    "    print(f\"   Ruim (<0.4): {poor} ({poor/len(results)*100:.1f}%)\")\n",
    "    \n",
    "    # Análise por bioma/estação\n",
    "    biome_stats = {}\n",
    "    season_stats = {}\n",
    "    \n",
    "    for result in results:\n",
    "        metadata = result['metadata']\n",
    "        score = result['overall_score']\n",
    "        \n",
    "        if metadata.get('biome'):\n",
    "            biome = metadata['biome']\n",
    "            if biome not in biome_stats:\n",
    "                biome_stats[biome] = []\n",
    "            biome_stats[biome].append(score)\n",
    "        \n",
    "        if metadata.get('season'):\n",
    "            season = metadata['season']\n",
    "            if season not in season_stats:\n",
    "                season_stats[season] = []\n",
    "            season_stats[season].append(score)\n",
    "    \n",
    "    if biome_stats:\n",
    "        print(f\"\\n🌿 Qualidade por Bioma:\")\n",
    "        for biome, scores in biome_stats.items():\n",
    "            print(f\"   {biome}: {np.mean(scores):.3f} (n={len(scores)})\")\n",
    "    \n",
    "    if season_stats:\n",
    "        print(f\"\\n🌦️ Qualidade por Estação:\")\n",
    "        for season, scores in season_stats.items():\n",
    "            print(f\"   {season}: {np.mean(scores):.3f} (n={len(scores)})\")\n",
    "    \n",
    "    # Visualização\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Histograma geral\n",
    "    axes[0,0].hist(overall_scores, bins=20, alpha=0.7, color='blue')\n",
    "    axes[0,0].axvline(np.mean(overall_scores), color='red', linestyle='--', label=f'Média: {np.mean(overall_scores):.3f}')\n",
    "    axes[0,0].set_xlabel('Score Geral')\n",
    "    axes[0,0].set_ylabel('Frequência')\n",
    "    axes[0,0].set_title('Distribuição de Qualidade Geral')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Comparação de métricas\n",
    "    metrics_data = [scores['technical'], scores['agricultural'], scores['seasonal']]\n",
    "    axes[0,1].boxplot(metrics_data, labels=['Técnica', 'Agronômica', 'Sazonal'])\n",
    "    axes[0,1].set_ylabel('Score')\n",
    "    axes[0,1].set_title('Comparação de Métricas')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Qualidade por bioma\n",
    "    if biome_stats:\n",
    "        biome_means = [np.mean(scores) for scores in biome_stats.values()]\n",
    "        axes[1,0].bar(biome_stats.keys(), biome_means, alpha=0.7, color='green')\n",
    "        axes[1,0].set_ylabel('Score Médio')\n",
    "        axes[1,0].set_title('Qualidade por Bioma')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Qualidade por estação\n",
    "    if season_stats:\n",
    "        season_means = [np.mean(scores) for scores in season_stats.values()]\n",
    "        axes[1,1].bar(season_stats.keys(), season_means, alpha=0.7, color='orange')\n",
    "        axes[1,1].set_ylabel('Score Médio')\n",
    "        axes[1,1].set_title('Qualidade por Estação')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Interface\n",
    "dataset_path_widget = widgets.Text(\n",
    "    value='/content/generated_dataset',\n",
    "    description='Dataset Path:'\n",
    ")\n",
    "\n",
    "sample_size_widget = widgets.IntSlider(\n",
    "    value=50,\n",
    "    min=10,\n",
    "    max=200,\n",
    "    description='Amostra:'\n",
    ")\n",
    "\n",
    "analyze_quality_btn = widgets.Button(\n",
    "    description='🔍 Analisar Qualidade',\n",
    "    button_style='info'\n",
    ")\n",
    "\n",
    "quality_output = widgets.Output()\n",
    "\n",
    "def quality_callback(btn):\n",
    "    with quality_output:\n",
    "        clear_output()\n",
    "        analyze_dataset_quality(dataset_path_widget.value, sample_size_widget.value)\n",
    "\n",
    "analyze_quality_btn.on_click(quality_callback)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>🔍 Análise de Qualidade</h3>\"),\n",
    "    dataset_path_widget,\n",
    "    sample_size_widget,\n",
    "    analyze_quality_btn,\n",
    "    quality_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Filtragem por Qualidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset_by_quality(dataset_path, quality_threshold=0.7, output_path=None):\n",
    "    \"\"\"Filtra dataset removendo imagens de baixa qualidade\"\"\"\n",
    "    \n",
    "    if not output_path:\n",
    "        output_path = f\"{dataset_path}_filtered\"\n",
    "    \n",
    "    dataset_dir = Path(dataset_path)\n",
    "    output_dir = Path(output_path)\n",
    "    \n",
    "    print(f\"🎯 Filtrando dataset com threshold {quality_threshold}...\")\n",
    "    print(f\"Origem: {dataset_dir}\")\n",
    "    print(f\"Destino: {output_dir}\")\n",
    "    \n",
    "    # Criar estrutura de saída\n",
    "    (output_dir / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "    (output_dir / \"metadata\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Avaliar e filtrar\n",
    "    quality_evaluator = QualityMetrics()\n",
    "    \n",
    "    image_files = list((dataset_dir / \"images\").glob(\"*.jpg\"))\n",
    "    kept_images = []\n",
    "    rejected_images = []\n",
    "    \n",
    "    for img_path in tqdm(image_files, desc=\"Filtrando por qualidade\"):\n",
    "        try:\n",
    "            # Carregar e avaliar\n",
    "            image = Image.open(img_path)\n",
    "            report = quality_evaluator.evaluate_image_quality(image)\n",
    "            \n",
    "            if report.overall_score >= quality_threshold:\n",
    "                # Copiar imagem\n",
    "                dest_img = output_dir / \"images\" / img_path.name\n",
    "                image.save(dest_img)\n",
    "                \n",
    "                # Copiar metadata se existir\n",
    "                metadata_src = dataset_dir / \"metadata\" / (img_path.stem + \".json\")\n",
    "                metadata_dest = output_dir / \"metadata\" / (img_path.stem + \".json\")\n",
    "                \n",
    "                if metadata_src.exists():\n",
    "                    import shutil\n",
    "                    shutil.copy2(metadata_src, metadata_dest)\n",
    "                \n",
    "                kept_images.append({\n",
    "                    'filename': img_path.name,\n",
    "                    'score': report.overall_score\n",
    "                })\n",
    "            else:\n",
    "                rejected_images.append({\n",
    "                    'filename': img_path.name,\n",
    "                    'score': report.overall_score\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Erro processando {img_path.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n✅ Filtragem concluída!\")\n",
    "    print(f\"   Mantidas: {len(kept_images)}/{len(image_files)} ({len(kept_images)/len(image_files)*100:.1f}%)\")\n",
    "    print(f\"   Rejeitadas: {len(rejected_images)} ({len(rejected_images)/len(image_files)*100:.1f}%)\")\n",
    "    \n",
    "    if kept_images:\n",
    "        avg_score = np.mean([img['score'] for img in kept_images])\n",
    "        print(f\"   Score médio das mantidas: {avg_score:.3f}\")\n",
    "    \n",
    "    # Salvar relatório\n",
    "    filter_report = {\n",
    "        'filter_threshold': quality_threshold,\n",
    "        'original_count': len(image_files),\n",
    "        'kept_count': len(kept_images),\n",
    "        'rejected_count': len(rejected_images),\n",
    "        'kept_images': kept_images,\n",
    "        'rejected_images': rejected_images\n",
    "    }\n",
    "    \n",
    "    with open(output_dir / \"filter_report.json\", 'w') as f:\n",
    "        json.dump(filter_report, f, indent=2)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Interface de filtragem\n",
    "filter_threshold_widget = widgets.FloatSlider(\n",
    "    value=0.7,\n",
    "    min=0.4,\n",
    "    max=0.9,\n",
    "    step=0.05,\n",
    "    description='Threshold:'\n",
    ")\n",
    "\n",
    "filter_output_widget = widgets.Text(\n",
    "    value='/content/filtered_dataset',\n",
    "    description='Output:'\n",
    ")\n",
    "\n",
    "filter_btn = widgets.Button(\n",
    "    description='🎯 Filtrar Dataset',\n",
    "    button_style='warning'\n",
    ")\n",
    "\n",
    "filter_output = widgets.Output()\n",
    "\n",
    "def filter_callback(btn):\n",
    "    with filter_output:\n",
    "        clear_output()\n",
    "        filter_dataset_by_quality(\n",
    "            dataset_path_widget.value,\n",
    "            filter_threshold_widget.value,\n",
    "            filter_output_widget.value\n",
    "        )\n",
    "\n",
    "filter_btn.on_click(filter_callback)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>🎯 Filtragem por Qualidade</h3>\"),\n",
    "    filter_threshold_widget,\n",
    "    filter_output_widget,\n",
    "    filter_btn,\n",
    "    filter_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Comparação com Dataset Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_synthetic_vs_real(synthetic_path, real_path=None, sample_size=30):\n",
    "    \"\"\"Compara qualidade entre datasets sintético e real\"\"\"\n",
    "    \n",
    "    if not real_path:\n",
    "        print(\"⚠️ Dataset real não fornecido - comparação não disponível\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📊 Comparando datasets...\")\n",
    "    print(f\"Sintético: {synthetic_path}\")\n",
    "    print(f\"Real: {real_path}\")\n",
    "    \n",
    "    quality_evaluator = QualityMetrics()\n",
    "    \n",
    "    # Analisar dataset sintético\n",
    "    synthetic_scores = []\n",
    "    synthetic_images = list(Path(synthetic_path).glob(\"images/*.jpg\"))[:sample_size]\n",
    "    \n",
    "    for img_path in tqdm(synthetic_images, desc=\"Analisando sintético\"):\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            report = quality_evaluator.evaluate_image_quality(image)\n",
    "            synthetic_scores.append(report.overall_score)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Analisar dataset real\n",
    "    real_scores = []\n",
    "    real_images = list(Path(real_path).glob(\"**/*.jpg\"))[:sample_size]\n",
    "    \n",
    "    if real_images:\n",
    "        for img_path in tqdm(real_images, desc=\"Analisando real\"):\n",
    "            try:\n",
    "                image = Image.open(img_path)\n",
    "                report = quality_evaluator.evaluate_image_quality(image)\n",
    "                real_scores.append(report.overall_score)\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Comparação\n",
    "    if synthetic_scores and real_scores:\n",
    "        print(f\"\\n📊 COMPARAÇÃO DE QUALIDADE\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"Sintético: {np.mean(synthetic_scores):.3f} ± {np.std(synthetic_scores):.3f}\")\n",
    "        print(f\"Real: {np.mean(real_scores):.3f} ± {np.std(real_scores):.3f}\")\n",
    "        \n",
    "        # Teste estatístico\n",
    "        from scipy import stats\n",
    "        t_stat, p_value = stats.ttest_ind(synthetic_scores, real_scores)\n",
    "        print(f\"\\nTeste t: p-value = {p_value:.4f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            print(\"Diferença estatisticamente significativa\")\n",
    "        else:\n",
    "            print(\"Sem diferença estatística significativa\")\n",
    "        \n",
    "        # Visualização\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(synthetic_scores, alpha=0.7, label='Sintético', bins=15)\n",
    "        plt.hist(real_scores, alpha=0.7, label='Real', bins=15)\n",
    "        plt.xlabel('Score de Qualidade')\n",
    "        plt.ylabel('Frequência')\n",
    "        plt.title('Distribuição de Qualidade')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.boxplot([synthetic_scores, real_scores], labels=['Sintético', 'Real'])\n",
    "        plt.ylabel('Score de Qualidade')\n",
    "        plt.title('Comparação Boxplot')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ Não foi possível realizar comparação\")\n",
    "\n",
    "# Interface de comparação\n",
    "real_path_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='/path/to/real/dataset',\n",
    "    description='Dataset Real:'\n",
    ")\n",
    "\n",
    "compare_btn = widgets.Button(\n",
    "    description='📊 Comparar',\n",
    "    button_style='info'\n",
    ")\n",
    "\n",
    "compare_output = widgets.Output()\n",
    "\n",
    "def compare_callback(btn):\n",
    "    with compare_output:\n",
    "        clear_output()\n",
    "        compare_synthetic_vs_real(\n",
    "            dataset_path_widget.value,\n",
    "            real_path_widget.value if real_path_widget.value else None\n",
    "        )\n",
    "\n",
    "compare_btn.on_click(compare_callback)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>📊 Comparação Sintético vs Real</h3>\"),\n",
    "    real_path_widget,\n",
    "    compare_btn,\n",
    "    compare_output\n",
    "]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}