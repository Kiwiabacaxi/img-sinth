{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# üß™ Teste de Gera√ß√£o de Imagens - SYNTH_IMAGE\n",
    "\n",
    "Este notebook testa a gera√ß√£o de imagens sint√©ticas de pastagens brasileiras no Google Colab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üì¶ Instala√ß√£o e Configura√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# Instalar depend√™ncias faltantes\n",
    "!pip install controlnet-aux ultralytics xformers --upgrade --quiet\n",
    "\n",
    "# Verificar instala√ß√£o GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA dispon√≠vel: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM dispon√≠vel: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## üîß Importa√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libraries"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configurar device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Usando device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_setup"
   },
   "source": [
    "## ü§ñ Carregamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "# Carregar modelo Stable Diffusion\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "print(\"Carregando modelo Stable Diffusion...\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    safety_checker=None,\n",
    "    requires_safety_checker=False\n",
    ").to(device)\n",
    "\n",
    "# Usar scheduler mais r√°pido\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# Otimiza√ß√£o para Colab (vers√£o compat√≠vel)\n",
    "if device == \"cuda\":\n",
    "    try:\n",
    "        pipe.enable_model_cpu_offload()\n",
    "        print(\"CPU offload habilitado\")\n",
    "    except:\n",
    "        print(\"CPU offload n√£o dispon√≠vel\")\n",
    "\n",
    "print(\"Modelo carregado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generation"
   },
   "source": [
    "## üé® Gera√ß√£o de Imagens de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_images"
   },
   "outputs": [],
   "source": [
    "# Prompts para pastagens brasileiras\n",
    "prompts = [\n",
    "    \"aerial view of brazilian pasture with cattle, green grass, realistic, high quality\",\n",
    "    \"tropical grassland in brazil, savanna landscape, cattle grazing, blue sky, photorealistic\",\n",
    "    \"brazilian ranch pasture, green fields, farm animals, countryside, detailed\",\n",
    "    \"cerrado landscape brazil, grass field, scattered trees, natural lighting\"\n",
    "]\n",
    "\n",
    "# Par√¢metros de gera√ß√£o\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "# Gerar imagens\n",
    "images = []\n",
    "for i, prompt in enumerate(prompts[:2]):  # Gerar apenas 2 para teste\n",
    "    print(f\"Gerando imagem {i+1}/2: {prompt[:50]}...\")\n",
    "    \n",
    "    image = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=\"low quality, blurry, distorted, ugly, bad anatomy\",\n",
    "        num_inference_steps=20,  # Reduzido para velocidade\n",
    "        guidance_scale=7.5,\n",
    "        width=512,\n",
    "        height=512,\n",
    "        generator=generator\n",
    "    ).images[0]\n",
    "    \n",
    "    images.append(image)\n",
    "    \n",
    "print(\"Imagens geradas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "display"
   },
   "source": [
    "## üì∏ Visualiza√ß√£o dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display_images"
   },
   "outputs": [],
   "source": [
    "# Exibir imagens geradas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "for i, (image, prompt) in enumerate(zip(images, prompts[:2])):\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].set_title(f\"Imagem {i+1}\\n{prompt[:40]}...\", fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Salvar imagens\n",
    "for i, image in enumerate(images):\n",
    "    image.save(f\"pastagem_brasileira_{i+1}.png\")\n",
    "    print(f\"Imagem {i+1} salva como: pastagem_brasileira_{i+1}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_controlnet"
   },
   "source": [
    "## üéØ Teste ControlNet (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "controlnet_test"
   },
   "outputs": [],
   "source": [
    "# Testar ControlNet se dispon√≠vel\n",
    "try:\n",
    "    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "    from controlnet_aux import CannyDetector\n",
    "    import cv2\n",
    "    \n",
    "    print(\"‚úÖ ControlNet dispon√≠vel!\")\n",
    "    \n",
    "    # Criar uma imagem de borda simples para teste\n",
    "    canny = CannyDetector()\n",
    "    \n",
    "    # Usar primeira imagem gerada como base\n",
    "    if images:\n",
    "        canny_image = canny(images[0])\n",
    "        \n",
    "        # Exibir resultado\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(images[0])\n",
    "        plt.title(\"Imagem Original\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(canny_image, cmap='gray')\n",
    "        plt.title(\"Bordas Canny\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"ControlNet testado com sucesso!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ControlNet n√£o dispon√≠vel: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro no teste ControlNet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_yolo"
   },
   "source": [
    "## üéØ Teste YOLO (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yolo_test"
   },
   "outputs": [],
   "source": "# Testar YOLO com Segmenta√ß√£o de Gram√≠neas\ntry:\n    from ultralytics import YOLO\n    import cv2\n    import numpy as np\n    \n    print(\"‚úÖ YOLO dispon√≠vel!\")\n    \n    # Carregar modelo de segmenta√ß√£o YOLO (YOLOv8 segmentation)\n    print(\"Carregando modelo YOLOv8 para segmenta√ß√£o...\")\n    model = YOLO('yolov8n-seg.pt')  # Modelo de segmenta√ß√£o\n    \n    # Testar segmenta√ß√£o na primeira imagem\n    if images:\n        print(\"Realizando segmenta√ß√£o da imagem...\")\n        results = model(images[0])\n        \n        # Obter resultado\n        result = results[0]\n        \n        # Verificar se h√° m√°scaras de segmenta√ß√£o\n        if result.masks is not None:\n            # Converter imagem para numpy\n            img_array = np.array(images[0])\n            \n            # Criar visualiza√ß√£o\n            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n            \n            # 1. Imagem original\n            axes[0, 0].imshow(images[0])\n            axes[0, 0].set_title(\"Imagem Original\")\n            axes[0, 0].axis('off')\n            \n            # 2. Detec√ß√µes com caixas\n            annotated_img = result.plot()\n            axes[0, 1].imshow(cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB))\n            axes[0, 1].set_title(\"Detec√ß√µes YOLO\")\n            axes[0, 1].axis('off')\n            \n            # 3. M√°scaras de segmenta√ß√£o combinadas\n            masks = result.masks.data.cpu().numpy()\n            combined_mask = np.zeros(masks[0].shape)\n            \n            # Classes relacionadas a gram√≠neas/vegeta√ß√£o\n            grass_classes = []\n            for i, (mask, cls, conf) in enumerate(zip(masks, result.boxes.cls, result.boxes.conf)):\n                class_name = model.names[int(cls)]\n                confidence = float(conf)\n                \n                # Filtrar classes relacionadas √† vegeta√ß√£o/gram√≠neas\n                vegetation_keywords = ['grass', 'plant', 'vegetation', 'field', 'lawn']\n                if any(keyword in class_name.lower() for keyword in vegetation_keywords) or confidence > 0.3:\n                    combined_mask += mask\n                    grass_classes.append((class_name, confidence))\n                    print(f\"Detectado: {class_name} (confian√ßa: {confidence:.2f})\")\n            \n            # Normalizar m√°scara\n            combined_mask = np.clip(combined_mask, 0, 1)\n            \n            axes[1, 0].imshow(combined_mask, cmap='green', alpha=0.8)\n            axes[1, 0].set_title(\"M√°scara de Gram√≠neas/Vegeta√ß√£o\")\n            axes[1, 0].axis('off')\n            \n            # 4. Overlay da m√°scara na imagem original\n            overlay = img_array.copy()\n            green_mask = np.zeros_like(img_array)\n            green_mask[:, :, 1] = combined_mask * 255  # Canal verde\n            \n            # Aplicar overlay\n            alpha = 0.4\n            overlay_result = cv2.addWeighted(img_array, 1-alpha, green_mask, alpha, 0)\n            \n            axes[1, 1].imshow(overlay_result)\n            axes[1, 1].set_title(\"Overlay: Gram√≠neas Detectadas\")\n            axes[1, 1].axis('off')\n            \n            plt.tight_layout()\n            plt.show()\n            \n            # Estat√≠sticas da segmenta√ß√£o\n            total_pixels = combined_mask.shape[0] * combined_mask.shape[1]\n            grass_pixels = np.sum(combined_mask > 0)\n            grass_percentage = (grass_pixels / total_pixels) * 100\n            \n            print(f\"\\nüìä An√°lise de Gram√≠neas:\")\n            print(f\"√Årea total da imagem: {total_pixels} pixels\")\n            print(f\"√Årea de gram√≠neas detectada: {grass_pixels} pixels\")\n            print(f\"Percentual de gram√≠neas: {grass_percentage:.1f}%\")\n            \n            if grass_classes:\n                print(f\"Classes detectadas: {grass_classes}\")\n            \n        else:\n            # Fallback: detec√ß√£o simples se n√£o houver m√°scaras\n            annotated_frame = result.plot()\n            plt.figure(figsize=(10, 8))\n            plt.imshow(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))\n            plt.title(\"Detec√ß√µes YOLO (sem segmenta√ß√£o)\")\n            plt.axis('off')\n            plt.show()\n            \n            print(\"‚ö†Ô∏è Modelo n√£o retornou m√°scaras de segmenta√ß√£o\")\n        \n        print(\"‚úÖ Segmenta√ß√£o YOLO conclu√≠da!\")\n    \nexcept ImportError as e:\n    print(f\"‚ùå YOLO n√£o dispon√≠vel: {e}\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Erro na segmenta√ß√£o YOLO: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "metrics"
   },
   "source": [
    "## üìä M√©tricas de Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## üå± Segmenta√ß√£o Avan√ßada de Gram√≠neas",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Segmenta√ß√£o espec√≠fica de gram√≠neas usando t√©cnicas de cor e textura\ndef segment_grass_advanced(image):\n    \"\"\"\n    Segmenta√ß√£o avan√ßada de gram√≠neas baseada em cor, textura e caracter√≠sticas visuais\n    \"\"\"\n    # Converter para numpy array\n    img_array = np.array(image)\n    \n    # Converter para HSV para melhor segmenta√ß√£o de cores\n    hsv = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)\n    \n    # Definir ranges de cor para gram√≠neas (tons de verde)\n    # Verde claro a escuro, considerando diferentes condi√ß√µes de ilumina√ß√£o\n    lower_green1 = np.array([35, 40, 40])   # Verde claro\n    upper_green1 = np.array([85, 255, 255])\n    \n    lower_green2 = np.array([25, 30, 30])   # Verde mais amarelado (grama seca)\n    upper_green2 = np.array([95, 255, 180])\n    \n    # Criar m√°scaras de cor\n    mask_green1 = cv2.inRange(hsv, lower_green1, upper_green1)\n    mask_green2 = cv2.inRange(hsv, lower_green2, upper_green2)\n    grass_mask = cv2.bitwise_or(mask_green1, mask_green2)\n    \n    # Opera√ß√µes morfol√≥gicas para refinar a m√°scara\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n    grass_mask = cv2.morphologyEx(grass_mask, cv2.MORPH_CLOSE, kernel)\n    grass_mask = cv2.morphologyEx(grass_mask, cv2.MORPH_OPEN, kernel)\n    \n    # Filtrar por √°rea (remover pequenos ru√≠dos)\n    contours, _ = cv2.findContours(grass_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    filtered_mask = np.zeros_like(grass_mask)\n    \n    min_area = 100  # √Årea m√≠nima para considerar como gram√≠nea\n    for contour in contours:\n        area = cv2.contourArea(contour)\n        if area > min_area:\n            cv2.fillPoly(filtered_mask, [contour], 255)\n    \n    return filtered_mask\n\n# Aplicar segmenta√ß√£o avan√ßada\nif images:\n    print(\"üå± Aplicando segmenta√ß√£o avan√ßada de gram√≠neas...\")\n    \n    # Segmentar primeira imagem\n    grass_mask = segment_grass_advanced(images[0])\n    \n    # Criar visualiza√ß√£o comparativa\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    \n    # Imagem original\n    axes[0, 0].imshow(images[0])\n    axes[0, 0].set_title(\"Imagem Original\")\n    axes[0, 0].axis('off')\n    \n    # M√°scara de gram√≠neas\n    axes[0, 1].imshow(grass_mask, cmap='Greens')\n    axes[0, 1].set_title(\"M√°scara de Gram√≠neas\")\n    axes[0, 1].axis('off')\n    \n    # Overlay colorido\n    img_array = np.array(images[0])\n    overlay = img_array.copy()\n    \n    # Aplicar cor verde nas √°reas de gram√≠nea\n    overlay[grass_mask > 0] = [0, 255, 0]  # Verde puro\n    blended = cv2.addWeighted(img_array, 0.7, overlay, 0.3, 0)\n    \n    axes[0, 2].imshow(blended)\n    axes[0, 2].set_title(\"Overlay: Gram√≠neas Destacadas\")\n    axes[0, 2].axis('off')\n    \n    # An√°lise por componentes de cor HSV\n    hsv = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)\n    \n    axes[1, 0].imshow(hsv[:,:,0], cmap='hsv')\n    axes[1, 0].set_title(\"Canal H (Matiz)\")\n    axes[1, 0].axis('off')\n    \n    axes[1, 1].imshow(hsv[:,:,1], cmap='gray')\n    axes[1, 1].set_title(\"Canal S (Satura√ß√£o)\")\n    axes[1, 1].axis('off')\n    \n    axes[1, 2].imshow(hsv[:,:,2], cmap='gray')\n    axes[1, 2].set_title(\"Canal V (Valor)\")\n    axes[1, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Estat√≠sticas detalhadas\n    total_pixels = grass_mask.shape[0] * grass_mask.shape[1]\n    grass_pixels = np.sum(grass_mask > 0)\n    grass_percentage = (grass_pixels / total_pixels) * 100\n    \n    print(f\"\\nüìä An√°lise Detalhada de Gram√≠neas:\")\n    print(f\"Resolu√ß√£o da imagem: {grass_mask.shape[1]}x{grass_mask.shape[0]}\")\n    print(f\"Total de pixels: {total_pixels:,}\")\n    print(f\"Pixels de gram√≠nea detectados: {grass_pixels:,}\")\n    print(f\"Cobertura de gram√≠neas: {grass_percentage:.2f}%\")\n    print(f\"√Årea de gram√≠neas: ~{(grass_pixels * 0.01):.0f} m¬≤ (estimativa)\")\n    \n    # An√°lise de qualidade da pastagem\n    if grass_percentage > 70:\n        quality = \"Excelente üü¢\"\n    elif grass_percentage > 50:\n        quality = \"Boa üü°\" \n    elif grass_percentage > 30:\n        quality = \"Regular üü†\"\n    else:\n        quality = \"Baixa üî¥\"\n    \n    print(f\"Qualidade da pastagem: {quality}\")\nelse:\n    print(\"‚ùå Nenhuma imagem dispon√≠vel para segmenta√ß√£o\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "performance_metrics"
   },
   "outputs": [],
   "source": [
    "# Informa√ß√µes de mem√≥ria\n",
    "if torch.cuda.is_available():\n",
    "    print(\"üìä Uso de Mem√≥ria GPU:\")\n",
    "    print(f\"Alocada: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"Reservada: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    print(f\"M√°xima alocada: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# Verificar tamanhos das imagens\n",
    "if images:\n",
    "    print(\"\\nüì∏ Informa√ß√µes das Imagens:\")\n",
    "    for i, image in enumerate(images):\n",
    "        print(f\"Imagem {i+1}: {image.size} - Modo: {image.mode}\")\n",
    "\n",
    "print(\"\\n‚úÖ Teste conclu√≠do com sucesso!\")\n",
    "print(\"\\nüéØ Pr√≥ximos passos:\")\n",
    "print(\"- Ajustar prompts para pastagens brasileiras espec√≠ficas\")\n",
    "print(\"- Implementar pipeline completo com ControlNet\")\n",
    "print(\"- Integrar detec√ß√£o YOLO para valida√ß√£o\")\n",
    "print(\"- Configurar dataset personalizado\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}