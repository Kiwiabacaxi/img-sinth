{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# üåæ Brazilian GrassClover: Synthetic Dataset Generation\n",
    "\n",
    "Este notebook implementa a metodologia do **GrassClover Dataset** adaptada para gram√≠neas forrageiras brasileiras.\n",
    "\n",
    "**Baseado em:** Skovsen et al. \"The GrassClover Image Dataset for Semantic and Hierarchical Species Understanding in Agriculture\" (CVPR Workshops, 2019)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## üì¶ Instala√ß√£o e Configura√ß√£o\n",
    "\n",
    "Instala√ß√£o das depend√™ncias necess√°rias seguindo a metodologia GrassClover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Depend√™ncias para gera√ß√£o sint√©tica no estilo GrassClover\n# IMPORTANTE: Instalar vers√µes compat√≠veis para evitar conflitos\n!pip install \"numpy<2.0\" --upgrade --quiet\n!pip install torch torchvision torchaudio --upgrade --quiet\n!pip install \"diffusers==0.24.0\" transformers accelerate --upgrade --quiet\n!pip install opencv-python-headless pillow matplotlib --upgrade --quiet\n!pip install scikit-image scipy albumentations --upgrade --quiet\n!pip install ultralytics segment-anything --upgrade --quiet\n!pip install rasterio shapely --quiet\n\n# Verificar GPU dispon√≠vel\nimport torch\nprint(f\"üöÄ PyTorch: {torch.__version__}\")\nprint(f\"üéØ CUDA dispon√≠vel: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"üñ•Ô∏è  GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n    \n# Verificar vers√µes cr√≠ticas\nimport numpy as np\nimport diffusers\nprint(f\"üì¶ NumPy: {np.__version__}\")\nprint(f\"üé® Diffusers: {diffusers.__version__}\")\nprint(\"‚úÖ Depend√™ncias verificadas!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## üîß Importa√ß√µes e Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from transformers import pipeline\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üîß Device configurado: {device}\")\n",
    "\n",
    "# Configurar matplotlib para alta qualidade\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Seeds para reprodutibilidade\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Configura√ß√£o inicial conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## üå± Defini√ß√£o das Classes - Estilo GrassClover Brasileiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes hier√°rquicas seguindo metodologia GrassClover para pastagens brasileiras\n",
    "GRASS_CLOVER_CLASSES = {\n",
    "    'background': {\n",
    "        'id': 0,\n",
    "        'color': (0, 0, 0),  # Preto\n",
    "        'name': 'Background',\n",
    "        'description': 'Fundo da imagem'\n",
    "    },\n",
    "    'soil': {\n",
    "        'id': 1,\n",
    "        'color': (139, 69, 19),  # Marrom\n",
    "        'name': 'Solo',\n",
    "        'description': 'Solo exposto ou entre plantas'\n",
    "    },\n",
    "    'brachiaria': {\n",
    "        'id': 2,\n",
    "        'color': (34, 139, 34),  # Verde floresta\n",
    "        'name': 'Brachiaria',\n",
    "        'description': 'Brachiaria spp. - Principal gram√≠nea forrageira'\n",
    "    },\n",
    "    'panicum': {\n",
    "        'id': 3,\n",
    "        'color': (50, 205, 50),  # Verde lima\n",
    "        'name': 'Panicum',\n",
    "        'description': 'Panicum spp. - Gram√≠nea de alto valor nutritivo'\n",
    "    },\n",
    "    'cynodon': {\n",
    "        'id': 4,\n",
    "        'color': (0, 255, 127),  # Verde primavera\n",
    "        'name': 'Cynodon', \n",
    "        'description': 'Cynodon spp. - Gram√≠nea resistente'\n",
    "    },\n",
    "    'leguminous': {\n",
    "        'id': 5,\n",
    "        'color': (255, 20, 147),  # Rosa profundo\n",
    "        'name': 'Leguminosas',\n",
    "        'description': 'Plantas fixadoras de nitrog√™nio (equivalente ao clover)'\n",
    "    },\n",
    "    'weeds': {\n",
    "        'id': 6,\n",
    "        'color': (255, 165, 0),  # Laranja\n",
    "        'name': 'Ervas Daninhas',\n",
    "        'description': 'Plantas invasoras e indesej√°veis'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Paleta de cores para visualiza√ß√£o\n",
    "CLASS_COLORS = [cls['color'] for cls in GRASS_CLOVER_CLASSES.values()]\n",
    "CLASS_NAMES = [cls['name'] for cls in GRASS_CLOVER_CLASSES.values()]\n",
    "NUM_CLASSES = len(GRASS_CLOVER_CLASSES)\n",
    "\n",
    "# Criar colormap personalizado\n",
    "cmap_grass = ListedColormap([np.array(color)/255.0 for color in CLASS_COLORS])\n",
    "\n",
    "print(f\"üìä {NUM_CLASSES} classes definidas:\")\n",
    "for name, info in GRASS_CLOVER_CLASSES.items():\n",
    "    print(f\"  {info['id']}: {info['name']} - {info['description']}\")\n",
    "\n",
    "# Visualizar paleta de cores\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 2))\n",
    "colors_array = np.array(CLASS_COLORS).reshape(1, -1, 3) / 255.0\n",
    "ax.imshow(colors_array, aspect='auto')\n",
    "ax.set_xticks(range(len(CLASS_NAMES)))\n",
    "ax.set_xticklabels(CLASS_NAMES, rotation=45, ha='right')\n",
    "ax.set_yticks([])\n",
    "ax.set_title('üé® Paleta de Classes - GrassClover Brasileiro')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## üé® Gerador de Imagens Sint√©ticas - Metodologia GrassClover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrazilianGrassCloverGenerator:\n",
    "    \"\"\"\n",
    "    Gerador de imagens sint√©ticas baseado na metodologia GrassClover\n",
    "    Adaptado para gram√≠neas forrageiras brasileiras\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_size=(512, 512), ground_sampling_distance=6):\n",
    "        self.image_size = image_size\n",
    "        self.gsd = ground_sampling_distance  # pixels per mm (4-8 conforme GrassClover)\n",
    "        \n",
    "        # Carregar modelo de gera√ß√£o\n",
    "        print(\"ü§ñ Carregando modelo Stable Diffusion...\")\n",
    "        model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "        self.pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "            safety_checker=None,\n",
    "            requires_safety_checker=False\n",
    "        ).to(device)\n",
    "        \n",
    "        self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "            self.pipe.scheduler.config\n",
    "        )\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            try:\n",
    "                self.pipe.enable_model_cpu_offload()\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print(\"‚úÖ Modelo carregado com sucesso!\")\n",
    "        \n",
    "        # Pool de prompts para diferentes componentes\n",
    "        self.soil_prompts = [\n",
    "            \"brown fertile soil, agricultural field, detailed soil texture, natural lighting\",\n",
    "            \"dark earth soil, farm ground, realistic soil surface, outdoor lighting\", \n",
    "            \"brazilian tropical soil, rich earth, agricultural land, natural texture\"\n",
    "        ]\n",
    "        \n",
    "        self.grass_prompts = {\n",
    "            'brachiaria': [\n",
    "                \"Brachiaria brizantha grass, tropical forage grass, dense green coverage, detailed grass blades\",\n",
    "                \"Brachiaria decumbens, brazilian pasture grass, lush green field, natural grass texture\",\n",
    "                \"Brachiaria humidicola, tropical grassland, thick grass coverage, realistic vegetation\"\n",
    "            ],\n",
    "            'panicum': [\n",
    "                \"Panicum maximum momba√ßa, tall tropical grass, vibrant green forage, detailed grass structure\",\n",
    "                \"Panicum tanz√¢nia grass, high quality forage, dense green coverage, natural lighting\",\n",
    "                \"Panicum massai, compact tropical grass, uniform green field, realistic grass texture\"\n",
    "            ],\n",
    "            'cynodon': [\n",
    "                \"Cynodon dactylon tifton, fine textured grass, uniform green coverage, detailed grass blades\",\n",
    "                \"coast-cross Cynodon grass, resistant tropical grass, dense green lawn, natural texture\"\n",
    "            ],\n",
    "            'leguminous': [\n",
    "                \"tropical legume plants, nitrogen fixing plants, broad leaves, mixed with grass\",\n",
    "                \"forage legumes, clover-like plants, green leafy plants, agricultural setting\"\n",
    "            ],\n",
    "            'weeds': [\n",
    "                \"agricultural weeds, invasive plants, mixed vegetation, undesirable plants\",\n",
    "                \"weed plants in pasture, unwanted vegetation, sparse growth, natural setting\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def generate_soil_base(self):\n",
    "        \"\"\"Gera imagem base do solo\"\"\"\n",
    "        prompt = random.choice(self.soil_prompts)\n",
    "        \n",
    "        soil_image = self.pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=\"plants, grass, vegetation, animals, objects, sky, water\",\n",
    "            num_inference_steps=20,\n",
    "            guidance_scale=7.0,\n",
    "            width=self.image_size[0],\n",
    "            height=self.image_size[1],\n",
    "            generator=torch.Generator(device=device).manual_seed(random.randint(0, 1000))\n",
    "        ).images[0]\n",
    "        \n",
    "        return soil_image\n",
    "    \n",
    "    def generate_plant_cutout(self, plant_type):\n",
    "        \"\"\"Gera recorte de planta individual\"\"\"\n",
    "        prompts = self.grass_prompts.get(plant_type, self.grass_prompts['brachiaria'])\n",
    "        prompt = random.choice(prompts)\n",
    "        \n",
    "        # Adicionar especifica√ß√µes para recorte\n",
    "        cutout_prompt = f\"{prompt}, isolated plant, white background, single plant specimen, detailed, high resolution\"\n",
    "        \n",
    "        plant_image = self.pipe(\n",
    "            prompt=cutout_prompt,\n",
    "            negative_prompt=\"multiple plants, animals, soil, background vegetation, low quality, blurry\",\n",
    "            num_inference_steps=25,\n",
    "            guidance_scale=8.0,\n",
    "            width=256,  # Menor para recortes\n",
    "            height=256,\n",
    "            generator=torch.Generator(device=device).manual_seed(random.randint(0, 1000))\n",
    "        ).images[0]\n",
    "        \n",
    "        return plant_image\n",
    "    \n",
    "    def create_plant_mask(self, plant_image, threshold=200):\n",
    "        \"\"\"Cria m√°scara para extrair planta do fundo branco\"\"\"\n",
    "        img_array = np.array(plant_image)\n",
    "        \n",
    "        # M√°scara baseada em fundo branco\n",
    "        white_mask = np.all(img_array > threshold, axis=2)\n",
    "        plant_mask = ~white_mask\n",
    "        \n",
    "        return plant_mask\n",
    "    \n",
    "    def place_plant_on_soil(self, soil_image, plant_image, plant_mask, position, scale=1.0):\n",
    "        \"\"\"Coloca planta recortada sobre o solo\"\"\"\n",
    "        soil_array = np.array(soil_image)\n",
    "        plant_array = np.array(plant_image)\n",
    "        \n",
    "        # Redimensionar planta se necess√°rio\n",
    "        if scale != 1.0:\n",
    "            new_size = (int(plant_array.shape[1] * scale), int(plant_array.shape[0] * scale))\n",
    "            plant_array = cv2.resize(plant_array, new_size)\n",
    "            plant_mask = cv2.resize(plant_mask.astype(np.uint8), new_size).astype(bool)\n",
    "        \n",
    "        x, y = position\n",
    "        h, w = plant_array.shape[:2]\n",
    "        \n",
    "        # Verificar limites\n",
    "        if x + w <= soil_array.shape[1] and y + h <= soil_array.shape[0]:\n",
    "            # Aplicar planta onde h√° m√°scara\n",
    "            soil_array[y:y+h, x:x+w][plant_mask] = plant_array[plant_mask]\n",
    "        \n",
    "        return Image.fromarray(soil_array)\n",
    "    \n",
    "    def generate_synthetic_scene(self, target_lai=2.0, composition=None):\n",
    "        \"\"\"\n",
    "        Gera cena sint√©tica completa seguindo metodologia GrassClover\n",
    "        \n",
    "        Args:\n",
    "            target_lai: Leaf Area Index desejado (0.5-4.0)\n",
    "            composition: Dict com propor√ß√£o de cada classe\n",
    "        \"\"\"\n",
    "        if composition is None:\n",
    "            composition = {\n",
    "                'brachiaria': 0.4,\n",
    "                'panicum': 0.3,\n",
    "                'cynodon': 0.15,\n",
    "                'leguminous': 0.1,\n",
    "                'weeds': 0.05\n",
    "            }\n",
    "        \n",
    "        print(f\"üå± Gerando cena sint√©tica (LAI: {target_lai:.1f})...\")\n",
    "        \n",
    "        # 1. Gerar solo base\n",
    "        scene = self.generate_soil_base()\n",
    "        \n",
    "        # 2. Criar m√°scara de segmenta√ß√£o\n",
    "        segmentation_mask = np.ones(self.image_size, dtype=np.uint8)  # Come√ßar com solo\n",
    "        \n",
    "        # 3. Calcular n√∫mero de plantas baseado no LAI\n",
    "        base_plants = int(target_lai * 20)  # Aproxima√ß√£o\n",
    "        \n",
    "        plant_positions = []\n",
    "        \n",
    "        # 4. Colocar plantas por tipo\n",
    "        for plant_type, proportion in composition.items():\n",
    "            num_plants = int(base_plants * proportion)\n",
    "            class_id = GRASS_CLOVER_CLASSES[plant_type]['id']\n",
    "            \n",
    "            print(f\"  Adicionando {num_plants} plantas de {plant_type}...\")\n",
    "            \n",
    "            for _ in range(num_plants):\n",
    "                # Gerar planta\n",
    "                plant = self.generate_plant_cutout(plant_type)\n",
    "                plant_mask = self.create_plant_mask(plant)\n",
    "                \n",
    "                # Posi√ß√£o aleat√≥ria\n",
    "                scale = random.uniform(0.5, 1.5)\n",
    "                x = random.randint(0, self.image_size[0] - int(256 * scale))\n",
    "                y = random.randint(0, self.image_size[1] - int(256 * scale))\n",
    "                \n",
    "                # Colocar na cena\n",
    "                scene = self.place_plant_on_soil(scene, plant, plant_mask, (x, y), scale)\n",
    "                \n",
    "                # Atualizar m√°scara de segmenta√ß√£o\n",
    "                scaled_size = (int(256 * scale), int(256 * scale))\n",
    "                scaled_mask = cv2.resize(plant_mask.astype(np.uint8), scaled_size).astype(bool)\n",
    "                \n",
    "                if x + scaled_size[0] <= self.image_size[0] and y + scaled_size[1] <= self.image_size[1]:\n",
    "                    segmentation_mask[y:y+scaled_size[1], x:x+scaled_size[0]][scaled_mask] = class_id\n",
    "                \n",
    "                plant_positions.append({\n",
    "                    'type': plant_type,\n",
    "                    'position': (x, y),\n",
    "                    'scale': scale,\n",
    "                    'class_id': class_id\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            'image': scene,\n",
    "            'segmentation_mask': segmentation_mask,\n",
    "            'plant_positions': plant_positions,\n",
    "            'composition': composition,\n",
    "            'lai': target_lai,\n",
    "            'metadata': {\n",
    "                'gsd': self.gsd,\n",
    "                'image_size': self.image_size,\n",
    "                'num_plants': len(plant_positions),\n",
    "                'generation_time': datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Gerador BrazilianGrassClover criado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## üöÄ Gera√ß√£o de Dataset Sint√©tico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar gerador\n",
    "generator = BrazilianGrassCloverGenerator(image_size=(512, 512))\n",
    "\n",
    "# Configura√ß√µes do dataset\n",
    "num_synthetic_images = 8  # Come√ßar com poucas para teste\n",
    "lai_range = (1.0, 3.5)  # Leaf Area Index vari√°vel\n",
    "\n",
    "# Composi√ß√µes vari√°veis para simular diferentes pastagens\n",
    "composition_variants = [\n",
    "    {'brachiaria': 0.6, 'panicum': 0.2, 'cynodon': 0.1, 'leguminous': 0.08, 'weeds': 0.02},  # Brachiaria dominante\n",
    "    {'brachiaria': 0.3, 'panicum': 0.5, 'cynodon': 0.1, 'leguminous': 0.07, 'weeds': 0.03},  # Panicum dominante\n",
    "    {'brachiaria': 0.2, 'panicum': 0.2, 'cynodon': 0.4, 'leguminous': 0.15, 'weeds': 0.05}, # Cynodon com leguminosas\n",
    "    {'brachiaria': 0.4, 'panicum': 0.3, 'cynodon': 0.2, 'leguminous': 0.05, 'weeds': 0.05}, # Misto equilibrado\n",
    "    {'brachiaria': 0.5, 'panicum': 0.15, 'cynodon': 0.15, 'leguminous': 0.1, 'weeds': 0.1}, # Com mais ervas daninhas\n",
    "]\n",
    "\n",
    "print(f\"üåæ Gerando {num_synthetic_images} imagens sint√©ticas...\")\n",
    "\n",
    "synthetic_dataset = []\n",
    "\n",
    "for i in range(num_synthetic_images):\n",
    "    print(f\"\\nüì∏ Gerando imagem {i+1}/{num_synthetic_images}...\")\n",
    "    \n",
    "    # Par√¢metros vari√°veis\n",
    "    target_lai = random.uniform(*lai_range)\n",
    "    composition = random.choice(composition_variants)\n",
    "    \n",
    "    print(f\"  LAI alvo: {target_lai:.2f}\")\n",
    "    print(f\"  Composi√ß√£o: {composition}\")\n",
    "    \n",
    "    try:\n",
    "        # Gerar cena sint√©tica\n",
    "        scene_data = generator.generate_synthetic_scene(\n",
    "            target_lai=target_lai,\n",
    "            composition=composition\n",
    "        )\n",
    "        \n",
    "        scene_data['scene_id'] = i\n",
    "        synthetic_dataset.append(scene_data)\n",
    "        \n",
    "        print(f\"  ‚úÖ Cena {i+1} gerada com {len(scene_data['plant_positions'])} plantas\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Erro ao gerar cena {i+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nüéâ Dataset sint√©tico criado com {len(synthetic_dataset)} imagens!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## üìä Visualiza√ß√£o do Dataset Sint√©tico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar algumas imagens do dataset\n",
    "if synthetic_dataset:\n",
    "    print(\"üìä Visualizando dataset sint√©tico...\")\n",
    "    \n",
    "    # Mostrar primeiras 4 imagens\n",
    "    num_show = min(4, len(synthetic_dataset))\n",
    "    \n",
    "    fig, axes = plt.subplots(num_show, 3, figsize=(15, 5 * num_show))\n",
    "    fig.suptitle('üåæ Dataset Sint√©tico - Estilo GrassClover Brasileiro', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    if num_show == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(num_show):\n",
    "        scene = synthetic_dataset[i]\n",
    "        \n",
    "        # 1. Imagem RGB\n",
    "        axes[i, 0].imshow(scene['image'])\n",
    "        axes[i, 0].set_title(f\"Cena {i+1}\\nLAI: {scene['lai']:.2f}\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # 2. M√°scara de segmenta√ß√£o\n",
    "        seg_colored = cmap_grass(scene['segmentation_mask'] / (NUM_CLASSES - 1))\n",
    "        axes[i, 1].imshow(seg_colored)\n",
    "        axes[i, 1].set_title(f\"Segmenta√ß√£o\\n{len(scene['plant_positions'])} plantas\")\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # 3. Overlay\n",
    "        img_array = np.array(scene['image'])\n",
    "        overlay = img_array * 0.7 + (seg_colored[:, :, :3] * 255) * 0.3\n",
    "        axes[i, 2].imshow(overlay.astype(np.uint8))\n",
    "        axes[i, 2].set_title('Overlay RGB + Segmenta√ß√£o')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Estat√≠sticas do dataset\n",
    "    print(\"\\nüìà Estat√≠sticas do Dataset:\")\n",
    "    \n",
    "    total_plants = sum(len(scene['plant_positions']) for scene in synthetic_dataset)\n",
    "    avg_lai = np.mean([scene['lai'] for scene in synthetic_dataset])\n",
    "    \n",
    "    print(f\"Total de imagens: {len(synthetic_dataset)}\")\n",
    "    print(f\"Total de plantas: {total_plants}\")\n",
    "    print(f\"Plantas por imagem: {total_plants/len(synthetic_dataset):.1f}\")\n",
    "    print(f\"LAI m√©dio: {avg_lai:.2f}\")\n",
    "    \n",
    "    # Contagem por classe\n",
    "    class_counts = Counter()\n",
    "    for scene in synthetic_dataset:\n",
    "        for plant in scene['plant_positions']:\n",
    "            class_counts[plant['type']] += 1\n",
    "    \n",
    "    print(\"\\nDistribui√ß√£o por classe:\")\n",
    "    for plant_type, count in class_counts.most_common():\n",
    "        percentage = (count / total_plants) * 100\n",
    "        print(f\"  {plant_type}: {count} plantas ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Gr√°fico de distribui√ß√£o\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Distribui√ß√£o LAI\n",
    "    lais = [scene['lai'] for scene in synthetic_dataset]\n",
    "    ax1.hist(lais, bins=10, alpha=0.7, color='green', edgecolor='black')\n",
    "    ax1.set_xlabel('Leaf Area Index (LAI)')\n",
    "    ax1.set_ylabel('Frequ√™ncia')\n",
    "    ax1.set_title('Distribui√ß√£o do LAI')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribui√ß√£o por classe\n",
    "    if class_counts:\n",
    "        classes = list(class_counts.keys())\n",
    "        counts = list(class_counts.values())\n",
    "        colors = [np.array(GRASS_CLOVER_CLASSES[cls]['color'])/255.0 for cls in classes]\n",
    "        \n",
    "        ax2.bar(classes, counts, color=colors)\n",
    "        ax2.set_xlabel('Tipo de Planta')\n",
    "        ax2.set_ylabel('N√∫mero de Plantas')\n",
    "        ax2.set_title('Distribui√ß√£o por Classe')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Nenhuma imagem sint√©tica foi gerada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## üîç An√°lise de Composi√ß√£o - Estilo GrassClover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_scene_composition(scene_data):\n",
    "    \"\"\"\n",
    "    An√°lise detalhada da composi√ß√£o da cena seguindo metodologia GrassClover\n",
    "    \"\"\"\n",
    "    seg_mask = scene_data['segmentation_mask']\n",
    "    total_pixels = seg_mask.shape[0] * seg_mask.shape[1]\n",
    "    \n",
    "    # Contagem por classe\n",
    "    class_pixels = {}\n",
    "    class_percentages = {}\n",
    "    \n",
    "    for class_name, class_info in GRASS_CLOVER_CLASSES.items():\n",
    "        class_id = class_info['id']\n",
    "        pixels = np.sum(seg_mask == class_id)\n",
    "        percentage = (pixels / total_pixels) * 100\n",
    "        \n",
    "        class_pixels[class_name] = pixels\n",
    "        class_percentages[class_name] = percentage\n",
    "    \n",
    "    # An√°lise de biomassa (similar ao GrassClover)\n",
    "    vegetation_pixels = total_pixels - class_pixels['background'] - class_pixels['soil']\n",
    "    vegetation_coverage = (vegetation_pixels / total_pixels) * 100\n",
    "    \n",
    "    # Densidade de plantas\n",
    "    plant_density = len(scene_data['plant_positions']) / (total_pixels / (1000 * 1000))  # plantas por m¬≤\n",
    "    \n",
    "    return {\n",
    "        'scene_id': scene_data['scene_id'],\n",
    "        'total_pixels': total_pixels,\n",
    "        'class_pixels': class_pixels,\n",
    "        'class_percentages': class_percentages,\n",
    "        'vegetation_coverage': vegetation_coverage,\n",
    "        'plant_density': plant_density,\n",
    "        'lai': scene_data['lai'],\n",
    "        'num_plants': len(scene_data['plant_positions'])\n",
    "    }\n",
    "\n",
    "# Analisar todas as cenas\n",
    "if synthetic_dataset:\n",
    "    print(\"üîç Analisando composi√ß√£o das cenas...\")\n",
    "    \n",
    "    composition_analyses = []\n",
    "    for scene in synthetic_dataset:\n",
    "        analysis = analyze_scene_composition(scene)\n",
    "        composition_analyses.append(analysis)\n",
    "    \n",
    "    # Visualizar an√°lises\n",
    "    num_analyses = len(composition_analyses)\n",
    "    \n",
    "    # Gr√°fico de cobertura por classe para cada cena\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('üìä An√°lise de Composi√ß√£o - Metodologia GrassClover', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Cobertura vegetacional por cena\n",
    "    scene_ids = [a['scene_id'] for a in composition_analyses]\n",
    "    vegetation_coverages = [a['vegetation_coverage'] for a in composition_analyses]\n",
    "    \n",
    "    axes[0, 0].bar(scene_ids, vegetation_coverages, color='green', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('ID da Cena')\n",
    "    axes[0, 0].set_ylabel('Cobertura Vegetacional (%)')\n",
    "    axes[0, 0].set_title('Cobertura Vegetacional por Cena')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. LAI vs Cobertura\n",
    "    lais = [a['lai'] for a in composition_analyses]\n",
    "    \n",
    "    axes[0, 1].scatter(lais, vegetation_coverages, c=scene_ids, cmap='viridis', s=100)\n",
    "    axes[0, 1].set_xlabel('Leaf Area Index (LAI)')\n",
    "    axes[0, 1].set_ylabel('Cobertura Vegetacional (%)')\n",
    "    axes[0, 1].set_title('Rela√ß√£o LAI vs Cobertura')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Composi√ß√£o m√©dia por classe\n",
    "    avg_percentages = {}\n",
    "    for class_name in GRASS_CLOVER_CLASSES.keys():\n",
    "        percentages = [a['class_percentages'][class_name] for a in composition_analyses]\n",
    "        avg_percentages[class_name] = np.mean(percentages)\n",
    "    \n",
    "    # Filtrar classes com cobertura significativa (> 0.1%)\n",
    "    significant_classes = {k: v for k, v in avg_percentages.items() if v > 0.1}\n",
    "    \n",
    "    if significant_classes:\n",
    "        class_names = list(significant_classes.keys())\n",
    "        percentages = list(significant_classes.values())\n",
    "        colors = [np.array(GRASS_CLOVER_CLASSES[cls]['color'])/255.0 for cls in class_names]\n",
    "        \n",
    "        axes[1, 0].pie(percentages, labels=class_names, colors=colors, autopct='%1.1f%%')\n",
    "        axes[1, 0].set_title('Composi√ß√£o M√©dia por Classe')\n",
    "    \n",
    "    # 4. Densidade de plantas vs LAI\n",
    "    plant_densities = [a['plant_density'] for a in composition_analyses]\n",
    "    \n",
    "    axes[1, 1].scatter(lais, plant_densities, c=vegetation_coverages, cmap='Greens', s=100)\n",
    "    axes[1, 1].set_xlabel('Leaf Area Index (LAI)')\n",
    "    axes[1, 1].set_ylabel('Densidade de Plantas (plantas/m¬≤)')\n",
    "    axes[1, 1].set_title('Densidade vs LAI')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.colorbar(axes[1, 1].collections[0], ax=axes[1, 1], label='Cobertura (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Relat√≥rio estat√≠stico\n",
    "    print(\"\\nüìà Relat√≥rio Estat√≠stico da Composi√ß√£o:\")\n",
    "    print(f\"Cobertura vegetacional m√©dia: {np.mean(vegetation_coverages):.1f}%\")\n",
    "    print(f\"LAI m√©dio: {np.mean(lais):.2f}\")\n",
    "    print(f\"Densidade m√©dia: {np.mean(plant_densities):.1f} plantas/m¬≤\")\n",
    "    \n",
    "    print(\"\\nComposi√ß√£o m√©dia por classe:\")\n",
    "    for class_name, avg_pct in avg_percentages.items():\n",
    "        if avg_pct > 0.1:  # Mostrar apenas classes significativas\n",
    "            print(f\"  {GRASS_CLOVER_CLASSES[class_name]['name']}: {avg_pct:.1f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado dispon√≠vel para an√°lise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## üß† Modelo de Segmenta√ß√£o - DeepLabV3+ (Estilo GrassClover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementa√ß√£o simplificada do DeepLabV3+ para segmenta√ß√£o\n",
    "# Seguindo a abordagem do paper GrassClover que usou Xception-65 based DeepLabv3+\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class SimpleDeepLabV3Plus(nn.Module):\n",
    "    \"\"\"\n",
    "    Vers√£o simplificada do DeepLabV3+ para segmenta√ß√£o de gram√≠neas\n",
    "    Inspirada na arquitetura usada no paper GrassClover\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=NUM_CLASSES, backbone='resnet50'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Backbone (encoder)\n",
    "        if backbone == 'resnet50':\n",
    "            self.backbone = models.resnet50(pretrained=True)\n",
    "            self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])  # Remover classifier\n",
    "            backbone_channels = 2048\n",
    "        \n",
    "        # ASPP (Atrous Spatial Pyramid Pooling)\n",
    "        self.aspp = ASPP(backbone_channels, 256)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = Decoder(num_classes)\n",
    "        \n",
    "        # Low-level features projection (skip connection)\n",
    "        self.low_level_conv = nn.Conv2d(256, 48, 1, bias=False)  # ResNet layer1 output\n",
    "        self.low_level_bn = nn.BatchNorm2d(48)\n",
    "        self.low_level_relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_size = x.shape[-2:]\n",
    "        \n",
    "        # Extrair features em diferentes escalas\n",
    "        features = self.extract_features(x)\n",
    "        \n",
    "        # ASPP\n",
    "        aspp_out = self.aspp(features['high_level'])\n",
    "        \n",
    "        # Upsample ASPP output\n",
    "        aspp_upsampled = F.interpolate(aspp_out, size=features['low_level'].shape[-2:], \n",
    "                                     mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Low-level features\n",
    "        low_level = self.low_level_conv(features['low_level'])\n",
    "        low_level = self.low_level_bn(low_level)\n",
    "        low_level = self.low_level_relu(low_level)\n",
    "        \n",
    "        # Concatenate\n",
    "        concat_features = torch.cat([aspp_upsampled, low_level], dim=1)\n",
    "        \n",
    "        # Decoder\n",
    "        output = self.decoder(concat_features)\n",
    "        \n",
    "        # Final upsample\n",
    "        output = F.interpolate(output, size=input_size, mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def extract_features(self, x):\n",
    "        \"\"\"Extrai features do backbone\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Forward atrav√©s das camadas do ResNet\n",
    "        x = self.backbone[0](x)  # conv1\n",
    "        x = self.backbone[1](x)  # bn1\n",
    "        x = self.backbone[2](x)  # relu\n",
    "        x = self.backbone[3](x)  # maxpool\n",
    "        \n",
    "        x = self.backbone[4](x)  # layer1\n",
    "        features['low_level'] = x  # Skip connection\n",
    "        \n",
    "        x = self.backbone[5](x)  # layer2\n",
    "        x = self.backbone[6](x)  # layer3\n",
    "        x = self.backbone[7](x)  # layer4\n",
    "        \n",
    "        features['high_level'] = x\n",
    "        \n",
    "        return features\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    \"\"\"Atrous Spatial Pyramid Pooling\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Different atrous rates\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
    "        self.conv6 = nn.Conv2d(in_channels, out_channels, 3, padding=6, dilation=6, bias=False)\n",
    "        self.conv12 = nn.Conv2d(in_channels, out_channels, 3, padding=12, dilation=12, bias=False)\n",
    "        self.conv18 = nn.Conv2d(in_channels, out_channels, 3, padding=18, dilation=18, bias=False)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.global_conv = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
    "        \n",
    "        # Batch norms and activations\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn6 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn12 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn18 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn_global = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Final projection\n",
    "        self.project = nn.Conv2d(out_channels * 5, out_channels, 1, bias=False)\n",
    "        self.project_bn = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        size = x.shape[-2:]\n",
    "        \n",
    "        # Different atrous convolutions\n",
    "        x1 = self.relu(self.bn1(self.conv1(x)))\n",
    "        x6 = self.relu(self.bn6(self.conv6(x)))\n",
    "        x12 = self.relu(self.bn12(self.conv12(x)))\n",
    "        x18 = self.relu(self.bn18(self.conv18(x)))\n",
    "        \n",
    "        # Global pooling branch\n",
    "        x_global = self.global_pool(x)\n",
    "        x_global = self.relu(self.bn_global(self.global_conv(x_global)))\n",
    "        x_global = F.interpolate(x_global, size=size, mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Concatenate all branches\n",
    "        x_concat = torch.cat([x1, x6, x12, x18, x_global], dim=1)\n",
    "        \n",
    "        # Project to final output\n",
    "        output = self.project(x_concat)\n",
    "        output = self.project_bn(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder do DeepLabV3+\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Primeiro bloco do decoder\n",
    "        self.conv1 = nn.Conv2d(256 + 48, 256, 3, padding=1, bias=False)  # ASPP + low-level\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(256, 256, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Classificador final\n",
    "        self.classifier = nn.Conv2d(256, num_classes, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Criar modelo\n",
    "print(\"üß† Criando modelo DeepLabV3+ para segmenta√ß√£o...\")\n",
    "model = SimpleDeepLabV3Plus(num_classes=NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "# Contar par√¢metros\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úÖ Modelo criado com {total_params:,} par√¢metros ({trainable_params:,} trein√°veis)\")\n",
    "\n",
    "# Teste do modelo com entrada dummy\n",
    "if device == \"cuda\" and torch.cuda.is_available():\n",
    "    with torch.no_grad():\n",
    "        dummy_input = torch.randn(1, 3, 512, 512).to(device)\n",
    "        output = model(dummy_input)\n",
    "        print(f\"üìä Sa√≠da do modelo: {output.shape}\")\n",
    "        print(f\"üíæ Mem√≥ria GPU usada: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Teste em GPU n√£o dispon√≠vel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## üéØ Avalia√ß√£o e M√©tricas - Metodologia GrassClover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_miou(pred_mask, true_mask, num_classes=NUM_CLASSES, ignore_index=0):\n",
    "    \"\"\"\n",
    "    Calcula mean Intersection over Union (mIoU)\n",
    "    M√©trica principal usada no paper GrassClover\n",
    "    \"\"\"\n",
    "    iou_per_class = []\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        if class_id == ignore_index:\n",
    "            continue\n",
    "            \n",
    "        pred_class = (pred_mask == class_id)\n",
    "        true_class = (true_mask == class_id)\n",
    "        \n",
    "        intersection = np.logical_and(pred_class, true_class).sum()\n",
    "        union = np.logical_or(pred_class, true_class).sum()\n",
    "        \n",
    "        if union == 0:\n",
    "            iou = 0.0  # Classe n√£o presente\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "        \n",
    "        iou_per_class.append(iou)\n",
    "    \n",
    "    miou = np.mean(iou_per_class)\n",
    "    return miou, iou_per_class\n",
    "\n",
    "\n",
    "def calculate_pixel_accuracy(pred_mask, true_mask):\n",
    "    \"\"\"Calcula acur√°cia pixel a pixel\"\"\"\n",
    "    correct = np.sum(pred_mask == true_mask)\n",
    "    total = pred_mask.size\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def evaluate_biomass_composition(pred_mask, true_mask):\n",
    "    \"\"\"\n",
    "    Avalia predi√ß√£o da composi√ß√£o de biomassa\n",
    "    Seguindo metodologia do GrassClover para agricultura\n",
    "    \"\"\"\n",
    "    pred_composition = {}\n",
    "    true_composition = {}\n",
    "    \n",
    "    total_pixels = pred_mask.size\n",
    "    \n",
    "    for class_name, class_info in GRASS_CLOVER_CLASSES.items():\n",
    "        class_id = class_info['id']\n",
    "        \n",
    "        pred_pixels = np.sum(pred_mask == class_id)\n",
    "        true_pixels = np.sum(true_mask == class_id)\n",
    "        \n",
    "        pred_composition[class_name] = (pred_pixels / total_pixels) * 100\n",
    "        true_composition[class_name] = (true_pixels / total_pixels) * 100\n",
    "    \n",
    "    # Calcular erro absoluto m√©dio na composi√ß√£o\n",
    "    mae_composition = np.mean([\n",
    "        abs(pred_composition[class_name] - true_composition[class_name])\n",
    "        for class_name in GRASS_CLOVER_CLASSES.keys()\n",
    "    ])\n",
    "    \n",
    "    return pred_composition, true_composition, mae_composition\n",
    "\n",
    "\n",
    "# Fun√ß√£o para criar m√°scara de predi√ß√£o simulada (para demonstra√ß√£o)\n",
    "def create_simulated_prediction(true_mask, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Cria uma predi√ß√£o simulada baseada na m√°scara verdadeira\n",
    "    Para demonstrar as m√©tricas de avalia√ß√£o\n",
    "    \"\"\"\n",
    "    pred_mask = true_mask.copy()\n",
    "    \n",
    "    # Adicionar ru√≠do aleat√≥rio\n",
    "    h, w = pred_mask.shape\n",
    "    noise_pixels = int(h * w * noise_level)\n",
    "    \n",
    "    for _ in range(noise_pixels):\n",
    "        y = random.randint(0, h-1)\n",
    "        x = random.randint(0, w-1)\n",
    "        \n",
    "        # Trocar para classe aleat√≥ria\n",
    "        available_classes = list(range(NUM_CLASSES))\n",
    "        available_classes.remove(pred_mask[y, x])  # Remover classe atual\n",
    "        if available_classes:\n",
    "            pred_mask[y, x] = random.choice(available_classes)\n",
    "    \n",
    "    return pred_mask\n",
    "\n",
    "\n",
    "# Avaliar dataset sint√©tico\n",
    "if synthetic_dataset:\n",
    "    print(\"üéØ Avaliando dataset com m√©tricas do GrassClover...\")\n",
    "    \n",
    "    evaluation_results = []\n",
    "    \n",
    "    # Avaliar primeiras 3 imagens como exemplo\n",
    "    num_eval = min(3, len(synthetic_dataset))\n",
    "    \n",
    "    for i in range(num_eval):\n",
    "        scene = synthetic_dataset[i]\n",
    "        true_mask = scene['segmentation_mask']\n",
    "        \n",
    "        # Criar predi√ß√£o simulada\n",
    "        pred_mask = create_simulated_prediction(true_mask, noise_level=0.15)\n",
    "        \n",
    "        # Calcular m√©tricas\n",
    "        miou, iou_per_class = calculate_miou(pred_mask, true_mask)\n",
    "        pixel_acc = calculate_pixel_accuracy(pred_mask, true_mask)\n",
    "        pred_comp, true_comp, mae_comp = evaluate_biomass_composition(pred_mask, true_mask)\n",
    "        \n",
    "        evaluation_results.append({\n",
    "            'scene_id': i,\n",
    "            'miou': miou,\n",
    "            'iou_per_class': iou_per_class,\n",
    "            'pixel_accuracy': pixel_acc,\n",
    "            'mae_composition': mae_comp,\n",
    "            'pred_composition': pred_comp,\n",
    "            'true_composition': true_comp\n",
    "        })\n",
    "        \n",
    "        print(f\"Cena {i+1}: mIoU = {miou:.3f}, Pixel Acc = {pixel_acc:.3f}, MAE Comp = {mae_comp:.2f}%\")\n",
    "    \n",
    "    # Visualizar resultados da avalia√ß√£o\n",
    "    fig, axes = plt.subplots(num_eval, 4, figsize=(20, 5 * num_eval))\n",
    "    fig.suptitle('üéØ Avalia√ß√£o de Segmenta√ß√£o - Metodologia GrassClover', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    if num_eval == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(num_eval):\n",
    "        scene = synthetic_dataset[i]\n",
    "        result = evaluation_results[i]\n",
    "        \n",
    "        true_mask = scene['segmentation_mask']\n",
    "        pred_mask = create_simulated_prediction(true_mask, noise_level=0.15)\n",
    "        \n",
    "        # 1. Imagem original\n",
    "        axes[i, 0].imshow(scene['image'])\n",
    "        axes[i, 0].set_title(f\"Cena {i+1} - Original\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # 2. Ground truth\n",
    "        gt_colored = cmap_grass(true_mask / (NUM_CLASSES - 1))\n",
    "        axes[i, 1].imshow(gt_colored)\n",
    "        axes[i, 1].set_title('Ground Truth')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # 3. Predi√ß√£o simulada\n",
    "        pred_colored = cmap_grass(pred_mask / (NUM_CLASSES - 1))\n",
    "        axes[i, 2].imshow(pred_colored)\n",
    "        axes[i, 2].set_title(f\"Predi√ß√£o\\nmIoU: {result['miou']:.3f}\")\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # 4. Mapa de erro\n",
    "        error_map = (pred_mask != true_mask).astype(np.uint8)\n",
    "        axes[i, 3].imshow(error_map, cmap='Reds')\n",
    "        axes[i, 3].set_title(f\"Erros\\nPixel Acc: {result['pixel_accuracy']:.3f}\")\n",
    "        axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Estat√≠sticas gerais\n",
    "    print(\"\\nüìä Estat√≠sticas de Avalia√ß√£o:\")\n",
    "    avg_miou = np.mean([r['miou'] for r in evaluation_results])\n",
    "    avg_pixel_acc = np.mean([r['pixel_accuracy'] for r in evaluation_results])\n",
    "    avg_mae_comp = np.mean([r['mae_composition'] for r in evaluation_results])\n",
    "    \n",
    "    print(f\"mIoU m√©dio: {avg_miou:.3f}\")\n",
    "    print(f\"Acur√°cia pixel m√©dia: {avg_pixel_acc:.3f}\")\n",
    "    print(f\"MAE composi√ß√£o m√©dia: {avg_mae_comp:.2f}%\")\n",
    "    \n",
    "    # IoU por classe\n",
    "    class_names_filtered = [name for name in CLASS_NAMES if name != 'Background']\n",
    "    avg_iou_per_class = np.mean([r['iou_per_class'] for r in evaluation_results], axis=0)\n",
    "    \n",
    "    print(\"\\nIoU por classe:\")\n",
    "    for i, (class_name, iou) in enumerate(zip(class_names_filtered, avg_iou_per_class)):\n",
    "        print(f\"  {class_name}: {iou:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìù Nota: O paper original GrassClover reportou mIoU de 0.55 com FCN-8s\")\n",
    "    print(f\"    Nosso resultado simulado: {avg_miou:.3f}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado dispon√≠vel para avalia√ß√£o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## üíæ Exporta√ß√£o do Dataset - Formato GrassClover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar dataset no formato compat√≠vel com metodologia GrassClover\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def export_grassclover_dataset(dataset, output_dir=\"grassclover_brazilian_dataset\"):\n",
    "    \"\"\"\n",
    "    Exporta dataset no formato GrassClover\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Estrutura de diret√≥rios\n",
    "    (output_path / \"images\").mkdir(exist_ok=True)\n",
    "    (output_path / \"masks\").mkdir(exist_ok=True)\n",
    "    (output_path / \"metadata\").mkdir(exist_ok=True)\n",
    "    \n",
    "    dataset_info = {\n",
    "        'name': 'Brazilian GrassClover Dataset',\n",
    "        'description': 'Synthetic dataset of Brazilian forage grasses following GrassClover methodology',\n",
    "        'created': datetime.now().isoformat(),\n",
    "        'num_images': len(dataset),\n",
    "        'image_size': dataset[0]['image'].size if dataset else [512, 512],\n",
    "        'classes': GRASS_CLOVER_CLASSES,\n",
    "        'methodology': 'Based on Skovsen et al. GrassClover Dataset (CVPR 2019)',\n",
    "        'ground_sampling_distance': '4-8 px/mm',\n",
    "        'scene_parameters': {\n",
    "            'lai_range': [1.0, 3.5],\n",
    "            'composition_variants': 5,\n",
    "            'plant_density_range': 'Variable based on LAI'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    exported_scenes = []\n",
    "    \n",
    "    print(f\"üíæ Exportando {len(dataset)} cenas para {output_path}...\")\n",
    "    \n",
    "    for i, scene in enumerate(dataset):\n",
    "        scene_id = f\"scene_{i:04d}\"\n",
    "        \n",
    "        # Salvar imagem RGB\n",
    "        image_path = output_path / \"images\" / f\"{scene_id}.png\"\n",
    "        scene['image'].save(image_path)\n",
    "        \n",
    "        # Salvar m√°scara de segmenta√ß√£o\n",
    "        mask_path = output_path / \"masks\" / f\"{scene_id}_mask.png\"\n",
    "        mask_image = Image.fromarray(scene['segmentation_mask'].astype(np.uint8))\n",
    "        mask_image.save(mask_path)\n",
    "        \n",
    "        # Salvar m√°scara colorida para visualiza√ß√£o\n",
    "        mask_colored_path = output_path / \"masks\" / f\"{scene_id}_colored.png\"\n",
    "        mask_colored = cmap_grass(scene['segmentation_mask'] / (NUM_CLASSES - 1))\n",
    "        mask_colored_image = Image.fromarray((mask_colored * 255).astype(np.uint8))\n",
    "        mask_colored_image.save(mask_colored_path)\n",
    "        \n",
    "        # Metadata da cena\n",
    "        scene_metadata = {\n",
    "            'scene_id': scene_id,\n",
    "            'lai': float(scene['lai']),\n",
    "            'composition': scene['composition'],\n",
    "            'num_plants': len(scene['plant_positions']),\n",
    "            'plant_positions': scene['plant_positions'],\n",
    "            'image_path': str(image_path.name),\n",
    "            'mask_path': str(mask_path.name),\n",
    "            'colored_mask_path': str(mask_colored_path.name),\n",
    "            'metadata': scene['metadata']\n",
    "        }\n",
    "        \n",
    "        # Salvar metadata individual\n",
    "        metadata_path = output_path / \"metadata\" / f\"{scene_id}.json\"\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(scene_metadata, f, indent=2)\n",
    "        \n",
    "        exported_scenes.append(scene_metadata)\n",
    "        \n",
    "        if (i + 1) % 2 == 0:\n",
    "            print(f\"  ‚úÖ {i+1} cenas exportadas\")\n",
    "    \n",
    "    # Salvar informa√ß√µes gerais do dataset\n",
    "    dataset_info['scenes'] = exported_scenes\n",
    "    \n",
    "    with open(output_path / \"dataset_info.json\", 'w') as f:\n",
    "        json.dump(dataset_info, f, indent=2)\n",
    "    \n",
    "    # Criar arquivo README\n",
    "    readme_content = f\"\"\"# Brazilian GrassClover Dataset\n",
    "\n",
    "## Descri√ß√£o\n",
    "Dataset sint√©tico de gram√≠neas forrageiras brasileiras seguindo a metodologia do GrassClover Dataset.\n",
    "\n",
    "## Refer√™ncia\n",
    "Baseado em: Skovsen et al. \"The GrassClover Image Dataset for Semantic and Hierarchical Species Understanding in Agriculture\" (CVPR Workshops, 2019)\n",
    "\n",
    "## Estrutura\n",
    "- `images/`: Imagens RGB sint√©ticas ({len(dataset)} imagens)\n",
    "- `masks/`: M√°scaras de segmenta√ß√£o pixel-perfect\n",
    "- `metadata/`: Metadados detalhados de cada cena\n",
    "- `dataset_info.json`: Informa√ß√µes gerais do dataset\n",
    "\n",
    "## Classes\n",
    "{chr(10).join([f\"- {info['id']}: {info['name']} - {info['description']}\" for info in GRASS_CLOVER_CLASSES.values()])}\n",
    "\n",
    "## Par√¢metros\n",
    "- Resolu√ß√£o: {dataset[0]['image'].size if dataset else '512x512'}\n",
    "- Ground Sampling Distance: 4-8 px/mm\n",
    "- LAI Range: 1.0-3.5\n",
    "- Total de cenas: {len(dataset)}\n",
    "\n",
    "## Uso\n",
    "Este dataset pode ser usado para:\n",
    "- Treinamento de modelos de segmenta√ß√£o sem√¢ntica\n",
    "- An√°lise de composi√ß√£o de biomassa\n",
    "- Estudos de pastagens brasileiras\n",
    "- Desenvolvimento de algoritmos de agricultura de precis√£o\n",
    "\n",
    "Gerado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\n",
    "\"\"\"\n",
    "    \n",
    "    with open(output_path / \"README.md\", 'w', encoding='utf-8') as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    print(f\"\\nüéâ Dataset exportado com sucesso para {output_path}!\")\n",
    "    print(f\"üìä Total: {len(dataset)} cenas com m√°scaras pixel-perfect\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "# Exportar dataset\n",
    "if synthetic_dataset:\n",
    "    dataset_path = export_grassclover_dataset(synthetic_dataset)\n",
    "    \n",
    "    # Mostrar estat√≠sticas finais\n",
    "    print(\"\\nüìà Estat√≠sticas Finais do Dataset:\")\n",
    "    \n",
    "    total_plants = sum(len(scene['plant_positions']) for scene in synthetic_dataset)\n",
    "    avg_lai = np.mean([scene['lai'] for scene in synthetic_dataset])\n",
    "    \n",
    "    print(f\"Total de imagens: {len(synthetic_dataset)}\")\n",
    "    print(f\"Total de plantas sint√©ticas: {total_plants}\")\n",
    "    print(f\"LAI m√©dio: {avg_lai:.2f}\")\n",
    "    \n",
    "    # An√°lise de composi√ß√£o final\n",
    "    all_compositions = []\n",
    "    for scene in synthetic_dataset:\n",
    "        for plant_type, proportion in scene['composition'].items():\n",
    "            all_compositions.append(plant_type)\n",
    "    \n",
    "    composition_counts = Counter(all_compositions)\n",
    "    print(\"\\nDistribui√ß√£o de tipos de pastagem:\")\n",
    "    for plant_type, count in composition_counts.most_common():\n",
    "        print(f\"  {GRASS_CLOVER_CLASSES[plant_type]['name']}: presente em {count} cenas\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset Brazilian GrassClover pronto para uso!\")\n",
    "    print(f\"üìÅ Localiza√ß√£o: {dataset_path.absolute()}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Nenhum dataset dispon√≠vel para exporta√ß√£o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## üìù Relat√≥rio Final e Conclus√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar relat√≥rio final seguindo padr√µes cient√≠ficos\n",
    "print(\"üìù RELAT√ìRIO FINAL - Brazilian GrassClover Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if synthetic_dataset:\n",
    "    print(f\"\\nüåæ DATASET GERADO:\")\n",
    "    print(f\"Metodologia: Baseada em Skovsen et al. (CVPR 2019)\")\n",
    "    print(f\"Total de imagens sint√©ticas: {len(synthetic_dataset)}\")\n",
    "    print(f\"Resolu√ß√£o: {synthetic_dataset[0]['image'].size}\")\n",
    "    print(f\"Classes: {NUM_CLASSES} (solo, gram√≠neas brasileiras, leguminosas, ervas)\")\n",
    "    \n",
    "    total_plants = sum(len(scene['plant_positions']) for scene in synthetic_dataset)\n",
    "    avg_plants_per_scene = total_plants / len(synthetic_dataset)\n",
    "    avg_lai = np.mean([scene['lai'] for scene in synthetic_dataset])\n",
    "    \n",
    "    print(f\"\\nüìä ESTAT√çSTICAS:\")\n",
    "    print(f\"Total de plantas sint√©ticas: {total_plants}\")\n",
    "    print(f\"Plantas por cena (m√©dia): {avg_plants_per_scene:.1f}\")\n",
    "    print(f\"Leaf Area Index m√©dio: {avg_lai:.2f}\")\n",
    "    print(f\"Ground Sampling Distance: 4-8 px/mm\")\n",
    "    \n",
    "    # An√°lise de composi√ß√£o\n",
    "    class_distribution = Counter()\n",
    "    for scene in synthetic_dataset:\n",
    "        for plant in scene['plant_positions']:\n",
    "            class_distribution[plant['type']] += 1\n",
    "    \n",
    "    print(f\"\\nüå± DISTRIBUI√á√ÉO POR CLASSE:\")\n",
    "    for plant_type, count in class_distribution.most_common():\n",
    "        percentage = (count / total_plants) * 100\n",
    "        class_name = GRASS_CLOVER_CLASSES[plant_type]['name']\n",
    "        print(f\"  {class_name}: {count} plantas ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Avalia√ß√£o simulada\n",
    "    if 'evaluation_results' in locals() and evaluation_results:\n",
    "        avg_miou = np.mean([r['miou'] for r in evaluation_results])\n",
    "        avg_pixel_acc = np.mean([r['pixel_accuracy'] for r in evaluation_results])\n",
    "        \n",
    "        print(f\"\\nüéØ M√âTRICAS DE AVALIA√á√ÉO (Simuladas):\")\n",
    "        print(f\"mIoU m√©dio: {avg_miou:.3f}\")\n",
    "        print(f\"Acur√°cia pixel m√©dia: {avg_pixel_acc:.3f}\")\n",
    "        print(f\"Compara√ß√£o: GrassClover original reportou mIoU=0.55\")\n",
    "\n",
    "print(f\"\\nüî¨ METODOLOGIA APLICADA:\")\n",
    "print(f\"‚úì Gera√ß√£o sint√©tica de plantas individuais\")\n",
    "print(f\"‚úì Composi√ß√£o sobre bases de solo realistas\")\n",
    "print(f\"‚úì Controle de Leaf Area Index (LAI)\")\n",
    "print(f\"‚úì M√°scaras de segmenta√ß√£o pixel-perfect\")\n",
    "print(f\"‚úì Varia√ß√µes de composi√ß√£o de esp√©cies\")\n",
    "print(f\"‚úì Simula√ß√£o de oclus√µes pesadas\")\n",
    "print(f\"‚úì Popula√ß√µes densas de gram√≠neas\")\n",
    "\n",
    "print(f\"\\nüåæ ADAPTA√á√ïES PARA PASTAGENS BRASILEIRAS:\")\n",
    "print(f\"‚úì Brachiaria spp. (brizantha, decumbens, humidicola)\")\n",
    "print(f\"‚úì Panicum spp. (momba√ßa, tanz√¢nia, massai)\")\n",
    "print(f\"‚úì Cynodon spp. (tifton, coast-cross)\")\n",
    "print(f\"‚úì Leguminosas fixadoras de nitrog√™nio\")\n",
    "print(f\"‚úì Ervas daninhas caracter√≠sticas\")\n",
    "\n",
    "print(f\"\\nüéØ APLICA√á√ïES POTENCIAIS:\")\n",
    "print(f\"‚Ä¢ Treinamento de modelos DeepLabV3+ para segmenta√ß√£o\")\n",
    "print(f\"‚Ä¢ An√°lise de composi√ß√£o de biomassa em pastagens\")\n",
    "print(f\"‚Ä¢ Monitoramento de qualidade de pastagens\")\n",
    "print(f\"‚Ä¢ Detec√ß√£o e quantifica√ß√£o de ervas daninhas\")\n",
    "print(f\"‚Ä¢ Agricultura de precis√£o para pecu√°ria\")\n",
    "print(f\"‚Ä¢ Estudos de biodiversidade em pastagens\")\n",
    "\n",
    "print(f\"\\nüìö REFER√äNCIAS E INSPIRA√á√ÉO:\")\n",
    "print(f\"[1] Skovsen et al. 'The GrassClover Image Dataset for Semantic\")\n",
    "print(f\"    and Hierarchical Species Understanding in Agriculture'\")\n",
    "print(f\"    IEEE/CVF CVPR Workshops, 2019\")\n",
    "print(f\"[2] Metodologia adaptada para gram√≠neas tropicais brasileiras\")\n",
    "print(f\"[3] Foco em esp√©cies forrageiras de import√¢ncia econ√¥mica\")\n",
    "\n",
    "print(f\"\\nüîÆ TRABALHOS FUTUROS:\")\n",
    "print(f\"‚Ä¢ Expans√£o para mais esp√©cies de gram√≠neas\")\n",
    "print(f\"‚Ä¢ Simula√ß√£o de condi√ß√µes clim√°ticas vari√°veis\")\n",
    "print(f\"‚Ä¢ Integra√ß√£o com dados de sensoriamento remoto\")\n",
    "print(f\"‚Ä¢ Valida√ß√£o com imagens reais de campo\")\n",
    "print(f\"‚Ä¢ Desenvolvimento de m√©tricas espec√≠ficas para pastagens\")\n",
    "\n",
    "print(f\"\\n‚ö° INFORMA√á√ïES T√âCNICAS:\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU utilizada: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Mem√≥ria GPU m√°xima: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Framework: PyTorch {torch.__version__}\")\n",
    "print(f\"Modelo de gera√ß√£o: Stable Diffusion v1.5\")\n",
    "print(f\"Tempo de processamento: Vari√°vel por imagem\")\n",
    "\n",
    "print(f\"\\nüèÅ CONCLUS√ÉO:\")\n",
    "print(f\"Dataset sint√©tico brasileiro criado com sucesso seguindo a metodologia\")\n",
    "print(f\"consolidada do GrassClover. O dataset captura a diversidade das\")\n",
    "print(f\"gram√≠neas forrageiras brasileiras e pode servir como base s√≥lida\")\n",
    "print(f\"para desenvolvimento de sistemas de vis√£o computacional aplicados\")\n",
    "print(f\"√† agricultura e pecu√°ria sustent√°vel no Brasil.\")\n",
    "\n",
    "print(f\"\\n‚úÖ Notebook executado com sucesso!\")\n",
    "print(f\"üìÖ Data de conclus√£o: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "print(f\"\\nüåæüáßüá∑ Brazilian GrassClover Dataset - Ready for Agriculture! üáßüá∑üåæ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}