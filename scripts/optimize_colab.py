#!/usr/bin/env python3
"""
Script de otimiza√ß√£o para Google Colab
Detecta configura√ß√£o do Colab e aplica otimiza√ß√µes apropriadas
"""

import os
import sys
import json
import psutil
import subprocess
from pathlib import Path
import logging

# Setup b√°sico de logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def detect_colab_environment():
    """
    Detecta se est√° rodando no Google Colab e qual tipo
    
    Returns:
        Dict com informa√ß√µes do ambiente
    """
    
    environment = {
        'is_colab': False,
        'colab_type': None,
        'gpu_available': False,
        'gpu_type': None,
        'gpu_memory_gb': 0,
        'ram_gb': 0,
        'disk_free_gb': 0
    }
    
    # Detectar Google Colab
    try:
        import google.colab
        environment['is_colab'] = True
        logger.info("‚úÖ Google Colab detectado")
    except ImportError:
        logger.info("‚ÑπÔ∏è N√£o est√° rodando no Google Colab")
        return environment
    
    # Detectar tipo de GPU
    try:
        import torch
        if torch.cuda.is_available():
            environment['gpu_available'] = True
            environment['gpu_type'] = torch.cuda.get_device_name(0)
            environment['gpu_memory_gb'] = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            
            # Classificar tipo de Colab baseado na GPU
            gpu_name = environment['gpu_type'].lower()
            if 'a100' in gpu_name or 'v100' in gpu_name:
                environment['colab_type'] = 'pro_plus'
            elif 'p100' in gpu_name or 'v100' in gpu_name:
                environment['colab_type'] = 'pro'
            elif 't4' in gpu_name:
                environment['colab_type'] = 'free_or_pro'
            else:
                environment['colab_type'] = 'unknown'
                
            logger.info(f"üñ•Ô∏è GPU: {environment['gpu_type']} ({environment['gpu_memory_gb']:.1f}GB)")
        else:
            logger.warning("‚ö†Ô∏è GPU n√£o dispon√≠vel")
    except ImportError:
        logger.warning("‚ö†Ô∏è PyTorch n√£o instalado")
    
    # Informa√ß√µes de sistema
    environment['ram_gb'] = psutil.virtual_memory().total / (1024**3)
    environment['disk_free_gb'] = psutil.disk_usage('.').free / (1024**3)
    
    logger.info(f"üíæ RAM: {environment['ram_gb']:.1f}GB")
    logger.info(f"üíΩ Disco livre: {environment['disk_free_gb']:.1f}GB")
    
    return environment

def optimize_memory_settings():
    """Aplica otimiza√ß√µes de mem√≥ria para Colab"""
    
    logger.info("üîß Aplicando otimiza√ß√µes de mem√≥ria...")
    
    # Configura√ß√µes de ambiente para PyTorch
    optimizations = {
        'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:512,garbage_collection_threshold:0.6',
        'CUDA_LAUNCH_BLOCKING': '0',  # Async para melhor performance
        'TOKENIZERS_PARALLELISM': 'false',  # Evitar warnings
    }
    
    for key, value in optimizations.items():
        os.environ[key] = value
        logger.info(f"   {key}={value}")
    
    # Configura√ß√µes Python
    try:
        import gc
        import torch
        
        # Garbage collection mais agressivo
        gc.collect()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            
            # Configura√ß√µes CUDA
            torch.backends.cudnn.benchmark = True  # Otimizar para inputs de tamanho fixo
            torch.backends.cuda.matmul.allow_tf32 = True  # TF32 para A100/H100
            
            logger.info("‚úÖ Otimiza√ß√µes CUDA aplicadas")
        
    except ImportError:
        logger.warning("‚ö†Ô∏è PyTorch n√£o dispon√≠vel para otimiza√ß√µes")

def install_system_dependencies():
    """Instala depend√™ncias do sistema necess√°rias"""
    
    logger.info("üì¶ Verificando depend√™ncias do sistema...")
    
    # Depend√™ncias APT necess√°rias
    apt_packages = [
        'ffmpeg',           # Para processamento de v√≠deo se necess√°rio
        'libsm6',           # Para OpenCV
        'libxext6',         # Para OpenCV
        'libxrender-dev',   # Para matplotlib
        'libglib2.0-0',     # Para OpenCV
        'libgl1-mesa-glx',  # Para OpenCV
    ]
    
    try:
        # Verificar se precisa instalar
        missing_packages = []
        for package in apt_packages:
            result = subprocess.run(['dpkg', '-l', package], 
                                  capture_output=True, text=True)
            if result.returncode != 0:
                missing_packages.append(package)
        
        if missing_packages:
            logger.info(f"Instalando pacotes: {missing_packages}")
            subprocess.run(['apt-get', 'update', '-qq'], check=True)
            subprocess.run(['apt-get', 'install', '-y', '-qq'] + missing_packages, 
                         check=True)
            logger.info("‚úÖ Depend√™ncias do sistema instaladas")
        else:
            logger.info("‚úÖ Depend√™ncias do sistema j√° instaladas")
            
    except subprocess.CalledProcessError as e:
        logger.warning(f"‚ö†Ô∏è Erro instalando depend√™ncias: {e}")
    except FileNotFoundError:
        logger.warning("‚ö†Ô∏è Sistema APT n√£o dispon√≠vel")

def optimize_pip_settings():
    """Otimiza configura√ß√µes do pip para Colab"""
    
    logger.info("üêç Otimizando configura√ß√µes pip...")
    
    # Configura√ß√µes pip otimizadas
    pip_config = """
[global]
cache-dir = /tmp/pip-cache
timeout = 60
retries = 3
trusted-host = pypi.org
                pypi.python.org
                files.pythonhosted.org

[install]
compile = false
no-warn-script-location = true
"""
    
    # Criar diret√≥rio de configura√ß√£o
    pip_config_dir = Path.home() / '.config' / 'pip'
    pip_config_dir.mkdir(parents=True, exist_ok=True)
    
    # Salvar configura√ß√£o
    config_file = pip_config_dir / 'pip.conf'
    with open(config_file, 'w') as f:
        f.write(pip_config)
    
    # Criar cache directory
    cache_dir = Path('/tmp/pip-cache')
    cache_dir.mkdir(exist_ok=True)
    
    logger.info("‚úÖ Configura√ß√µes pip otimizadas")

def setup_huggingface_cache():
    """Configura cache otimizado para Hugging Face"""
    
    logger.info("ü§ó Configurando cache Hugging Face...")
    
    # Usar /tmp para cache (mais espa√ßo no Colab)
    cache_dir = Path('/tmp/hf_cache')
    cache_dir.mkdir(exist_ok=True)
    
    os.environ['HF_HOME'] = str(cache_dir)
    os.environ['TRANSFORMERS_CACHE'] = str(cache_dir / 'transformers')
    os.environ['HF_DATASETS_CACHE'] = str(cache_dir / 'datasets')
    
    logger.info(f"‚úÖ Cache HF configurado: {cache_dir}")

def apply_colab_specific_optimizations(environment):
    """Aplica otimiza√ß√µes espec√≠ficas baseadas no tipo de Colab"""
    
    colab_type = environment.get('colab_type', 'unknown')
    gpu_memory = environment.get('gpu_memory_gb', 0)
    
    logger.info(f"üéØ Aplicando otimiza√ß√µes para: {colab_type}")
    
    # Configura√ß√µes por tipo
    if colab_type == 'pro_plus':
        # Colab Pro+ com A100
        config = {
            'max_batch_size': 32,
            'use_attention_slicing': False,
            'use_cpu_offload': False,
            'enable_xformers': True,
            'mixed_precision': True
        }
    elif colab_type == 'pro':
        # Colab Pro com V100/P100
        config = {
            'max_batch_size': 16,
            'use_attention_slicing': False,
            'use_cpu_offload': False,
            'enable_xformers': True,
            'mixed_precision': True
        }
    elif gpu_memory >= 15:  # T4 com boa mem√≥ria
        config = {
            'max_batch_size': 8,
            'use_attention_slicing': True,
            'use_cpu_offload': False,
            'enable_xformers': True,
            'mixed_precision': True
        }
    elif gpu_memory >= 8:   # T4 b√°sico
        config = {
            'max_batch_size': 4,
            'use_attention_slicing': True,
            'use_cpu_offload': True,
            'enable_xformers': True,
            'mixed_precision': True
        }
    else:  # GPU pequena ou CPU
        config = {
            'max_batch_size': 2,
            'use_attention_slicing': True,
            'use_cpu_offload': True,
            'enable_xformers': False,
            'mixed_precision': False
        }
    
    # Salvar configura√ß√£o otimizada
    config_path = Path('/tmp/colab_optimization_config.json')
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    logger.info("‚úÖ Configura√ß√£o otimizada salva")
    
    # Log das configura√ß√µes aplicadas
    for key, value in config.items():
        logger.info(f"   {key}: {value}")
    
    return config

def setup_monitoring():
    """Configura monitoramento de recursos"""
    
    logger.info("üìä Configurando monitoramento...")
    
    monitoring_script = """
import psutil
import GPUtil
import time

def monitor_resources():
    '''Monitor de recursos em tempo real'''
    try:
        # CPU e RAM
        cpu_percent = psutil.cpu_percent()
        memory = psutil.virtual_memory()
        
        print(f"CPU: {cpu_percent}% | RAM: {memory.percent}% ({memory.available/1e9:.1f}GB livre)")
        
        # GPU se dispon√≠vel
        gpus = GPUtil.getGPUs()
        if gpus:
            for gpu in gpus:
                print(f"GPU: {gpu.load*100:.1f}% | VRAM: {gpu.memoryUtil*100:.1f}% ({gpu.memoryFree}MB livre)")
        
    except Exception as e:
        print(f"Erro no monitoramento: {e}")

# Fun√ß√£o para usar nos notebooks
def check_resources():
    monitor_resources()
"""
    
    # Salvar script de monitoramento
    monitor_file = Path('/tmp/resource_monitor.py')
    with open(monitor_file, 'w') as f:
        f.write(monitoring_script)
    
    logger.info(f"‚úÖ Monitor salvo: {monitor_file}")

def cleanup_colab():
    """Limpeza de espa√ßo no Colab"""
    
    logger.info("üßπ Limpando espa√ßo no Colab...")
    
    # Diret√≥rios para limpar
    cleanup_paths = [
        '/tmp/*',
        '/content/sample_data',
        '/root/.cache/pip',
        '/var/log/*'
    ]
    
    freed_space = 0
    
    for path_pattern in cleanup_paths:
        try:
            if '*' in path_pattern:
                # Usar shell para wildcards
                result = subprocess.run(f'du -sb {path_pattern} 2>/dev/null || true', 
                                      shell=True, capture_output=True, text=True)
                if result.stdout:
                    size = sum(int(line.split()[0]) for line in result.stdout.strip().split('\n') if line)
                    freed_space += size
                
                subprocess.run(f'rm -rf {path_pattern}', shell=True)
            else:
                path = Path(path_pattern)
                if path.exists():
                    if path.is_file():
                        freed_space += path.stat().st_size
                        path.unlink()
                    else:
                        # Calcular tamanho do diret√≥rio
                        total_size = sum(f.stat().st_size for f in path.rglob('*') if f.is_file())
                        freed_space += total_size
                        subprocess.run(['rm', '-rf', str(path)])
        except Exception as e:
            logger.warning(f"Erro limpando {path_pattern}: {e}")
    
    freed_gb = freed_space / (1024**3)
    logger.info(f"‚úÖ Espa√ßo liberado: {freed_gb:.1f}GB")

def main():
    """Fun√ß√£o principal de otimiza√ß√£o"""
    
    print("üå± Otimizador do Brazilian Pasture Synthesis para Google Colab")
    print("=" * 60)
    
    # 1. Detectar ambiente
    environment = detect_colab_environment()
    
    if not environment['is_colab']:
        logger.info("‚ÑπÔ∏è N√£o est√° no Colab. Aplicando otimiza√ß√µes gerais...")
    
    # 2. Limpeza inicial
    cleanup_colab()
    
    # 3. Instalar depend√™ncias do sistema
    install_system_dependencies()
    
    # 4. Otimizar pip
    optimize_pip_settings()
    
    # 5. Configurar caches
    setup_huggingface_cache()
    
    # 6. Otimiza√ß√µes de mem√≥ria
    optimize_memory_settings()
    
    # 7. Aplicar otimiza√ß√µes espec√≠ficas
    config = apply_colab_specific_optimizations(environment)
    
    # 8. Configurar monitoramento
    setup_monitoring()
    
    print("\n‚úÖ OTIMIZA√á√ÉO CONCLU√çDA!")
    print("=" * 30)
    
    # Resumo das otimiza√ß√µes aplicadas
    print(f"üñ•Ô∏è Ambiente: {environment.get('colab_type', 'local')}")
    if environment['gpu_available']:
        print(f"üéÆ GPU: {environment['gpu_type']}")
    print(f"üì¶ Batch size recomendado: {config['max_batch_size']}")
    print(f"üíæ Attention slicing: {config['use_attention_slicing']}")
    print(f"‚ö° XFormers: {config['enable_xformers']}")
    
    # Instru√ß√µes para uso
    print("\nüìã PR√ìXIMOS PASSOS:")
    print("1. Execute: !python setup_colab.py")
    print("2. Use as configura√ß√µes otimizadas nos notebooks")
    print("3. Monitore recursos com: exec(open('/tmp/resource_monitor.py').read())")
    
    return environment, config

if __name__ == "__main__":
    main()